#!/usr/bin/env bash
# ------------------------------------------------------------
# Renda Cloud Lab Â· post-recreate.sh
# éœ€è¦ä½¿ç”¨ Terraform æˆåŠŸå¯åŠ¨äº†åŸºç¡€è®¾æ–½ï¼ˆNAT + ALB + EKS + IRSAï¼‰åï¼Œ
# å†ä½¿ç”¨æœ¬è„šæœ¬è¿›è¡Œéƒ¨ç½²å±‚çš„è‡ªåŠ¨åŒ–æ“ä½œã€‚
# ç¡®ä¿å°†é›†ç¾¤èµ„æºçš„åˆ›å»ºä¸ Kubernetes æœåŠ¡çš„éƒ¨ç½²è¿›è¡Œè§£è€¦ã€‚
# åŠŸèƒ½ï¼š
#   1. æ›´æ–°æœ¬åœ° kubeconfig ä»¥è¿æ¥æœ€æ–°åˆ›å»ºçš„é›†ç¾¤
#   2. é€šè¿‡ Helm å®‰è£…æˆ–å‡çº§ AWS Load Balancer Controller
#   3. é€šè¿‡ Helm å®‰è£…æˆ–å‡çº§ ${AUTOSCALER_RELEASE_NAME}
#   4. æ£€æŸ¥ NAT ç½‘å…³ã€ALBã€EKS æ§åˆ¶é¢å’ŒèŠ‚ç‚¹ç»„ç­‰çŠ¶æ€
#   5. è·å–æœ€æ–°çš„ EKS NodeGroup ç”Ÿæˆçš„ ASG åç§°
#   6. è‹¥ä¹‹å‰æœªç»‘å®šï¼Œåˆ™ä¸ºè¯¥ ASG é…ç½® SNS Spot Interruption é€šçŸ¥
#   7. è‡ªåŠ¨å†™å…¥ç»‘å®šæ—¥å¿—ï¼Œé¿å…é‡å¤æ‰§è¡Œ
#   8. éƒ¨ç½² task-apiï¼ˆå›ºå®š ECR digestï¼Œé…ç½®æ¢é’ˆ/èµ„æºï¼‰å¹¶åœ¨é›†ç¾¤å†…å†’çƒŸ
#   9. å‘å¸ƒ Ingressï¼Œç­‰å¾…å…¬ç½‘ ALB å°±ç»ªå¹¶åš HTTP å†’çƒŸ
#  10. å®‰è£… metrics-serverï¼ˆ--kubelet-insecure-tlsï¼‰
#  11. éƒ¨ç½² HPAï¼ˆCPU 60%ï¼Œmin=2/max=10ï¼Œå« behaviorï¼‰
# ä½¿ç”¨ï¼š
#   bash scripts/post-recreate.sh
# ------------------------------------------------------------

set -euo pipefail

# === å¯é…ç½®å‚æ•° ===
CLOUD_PROVIDER="aws"
# å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
PROFILE=${AWS_PROFILE:-phase2-sso}
REGION=${REGION:-us-east-1}
ACCOUNT_ID=${ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --profile "$PROFILE" --output text)}
echo "ä½¿ç”¨ AWS è´¦å·: $ACCOUNT_ID"

CLUSTER_NAME="dev"
NODEGROUP_NAME="ng-mixed"
KUBE_DEFAULT_NAMESPACE="kube-system"
ASG_PREFIX="eks-${NODEGROUP_NAME}"

# === åº”ç”¨éƒ¨ç½²å‚æ•°ï¼ˆå¯è¢«ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰===
# k8s å‘½åç©ºé—´ï¼ˆéœ€ä¸æ¸…å•ä¸­çš„ metadata.namespace ä¸€è‡´ï¼‰
NS="${NS:-svc-task}"
# Deployment/Service çš„åç§°ä¸å®¹å™¨å
APP="${APP:-task-api}"
# ECR ä»“åº“å
ECR_REPO="${ECR_REPO:-task-api}"
# è¦éƒ¨ç½²çš„é•œåƒ tagï¼ˆä¹Ÿå¯ç”¨ latestï¼‰ã€‚è‹¥è®¾ç½® IMAGE_DIGEST åˆ™ä¼˜å…ˆç”Ÿæ•ˆã€‚
IMAGE_TAG="${IMAGE_TAG:-0.1.0}"
# k8s æ¸…å•æ‰€åœ¨ç›®å½•ï¼ˆns-sa.yaml / configmap.yaml / deploy-svc.yamlï¼‰
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
ROOT_DIR="$(cd "${SCRIPT_DIR}/.." && pwd)"
K8S_BASE_DIR="${K8S_BASE_DIR:-${ROOT_DIR}/task-api/k8s/base}"
# è‹¥ä½ æƒ³å›ºå®šæŸä¸ª digestï¼Œå¯åœ¨è¿è¡Œå‰ export IMAGE_DIGEST=sha256:...

# ä¸º ASG é…ç½® Spot Interruption é€šçŸ¥çš„å‚æ•°
TOPIC_NAME="spot-interruption-topic"
TOPIC_ARN="arn:${CLOUD_PROVIDER}:sns:${REGION}:${ACCOUNT_ID}:${TOPIC_NAME}"
STATE_FILE="${SCRIPT_DIR}/.last-asg-bound"
# ASG ç›¸å…³å‚æ•°
AUTOSCALER_CHART_NAME="cluster-autoscaler"
AUTOSCALER_RELEASE_NAME=${AUTOSCALER_CHART_NAME}
AUTOSCALER_ROLE_NAME="eks-cluster-autoscaler"
AUTOSCALER_ROLE_ARN="arn:${CLOUD_PROVIDER}:iam::${ACCOUNT_ID}:role/${AUTOSCALER_ROLE_NAME}"
DEPLOYMENT_AUTOSCALER_NAME="${AUTOSCALER_RELEASE_NAME}-${CLOUD_PROVIDER}-${AUTOSCALER_CHART_NAME}"
POD_AUTOSCALER_LABEL="app.kubernetes.io/name=${AUTOSCALER_RELEASE_NAME}"

# AWS Load Balancer Controller settings
ALBC_CHART_NAME="aws-load-balancer-controller"
ALBC_RELEASE_NAME=${ALBC_CHART_NAME}
ALBC_SERVICE_ACCOUNT_NAME=${ALBC_CHART_NAME}
ALBC_CHART_VERSION="1.8.1"
ALBC_IMAGE_TAG="v2.8.1"
ALBC_IMAGE_REPO="602401143452.dkr.ecr.${REGION}.amazonaws.com/amazon/aws-load-balancer-controller"
ALBC_HELM_REPO_NAME="eks"
ALBC_HELM_REPO_URL="https://aws.github.io/eks-charts"
POD_ALBC_LABEL="app.kubernetes.io/name=${ALBC_RELEASE_NAME}"
# ---- Ingress ----
ING_FILE="${ROOT_DIR}/task-api/k8s/ingress.yaml"
# ---- HPA ----
HPA_FILE="${ROOT_DIR}/task-api/k8s/hpa.yaml"

# === å‡½æ•°å®šä¹‰ ===
# log() {
#   echo "[$(date '+%Y-%m-%d %H:%M:%S')] $*"
# }
log() {
  printf "[%s] %s\n" "$(date '+%H:%M:%S')" "$*";
}
abort() {
  printf "[%s] âŒ %s\n" "$(date '+%H:%M:%S')" "$*" >&2; exit 1;
}

# åˆ¤æ–­ EKS é›†ç¾¤æ˜¯å¦å­˜åœ¨
cluster_exists() {
  aws eks describe-cluster \
    --name "$CLUSTER_NAME" \
    --region "$REGION" \
    --profile "$PROFILE" >/dev/null 2>&1
}

# æ›´æ–° kubeconfig ä»¥è¿æ¥ EKS é›†ç¾¤
update_kubeconfig() {
  log "ğŸ”„ æ›´æ–° kubeconfig ä»¥è¿æ¥ EKS é›†ç¾¤: $CLUSTER_NAME"
  aws eks update-kubeconfig \
    --region "$REGION" \
    --name "$CLUSTER_NAME" \
    --profile "$PROFILE"
}

# æ£€æŸ¥ Cluster Autoscaler éƒ¨ç½²çŠ¶æ€
check_autoscaler_status() {
  if ! kubectl -n $KUBE_DEFAULT_NAMESPACE get deployment $DEPLOYMENT_AUTOSCALER_NAME >/dev/null 2>&1; then
    echo "missing"
    return
  fi
  if kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_AUTOSCALER_LABEL \
      --no-headers 2>/dev/null | grep -v Running >/dev/null; then
    echo "unhealthy"
  else
    echo "healthy"
  fi
}

# æ£€æŸ¥ AWS Load Balancer Controller éƒ¨ç½²çŠ¶æ€
check_albc_status() {
  if ! kubectl -n $KUBE_DEFAULT_NAMESPACE get deployment $ALBC_RELEASE_NAME >/dev/null 2>&1; then
    echo "missing"
    return
  fi
  if kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_ALBC_LABEL \
      --no-headers 2>/dev/null | grep -v Running >/dev/null; then
    echo "unhealthy"
  else
    echo "healthy"
  fi
}

# å®‰è£…æˆ–å‡çº§ AWS Load Balancer Controller
install_albc_controller() {
  local status
  status=$(check_albc_status)
  case "$status" in
    healthy)
      log "âœ… AWS Load Balancer Controller å·²æ­£å¸¸è¿è¡Œ, è·³è¿‡ Helm éƒ¨ç½²"
      return 0
      ;;
    missing)
      log "âš™ï¸  æ£€æµ‹åˆ° AWS Load Balancer Controller æœªéƒ¨ç½², å¼€å§‹å®‰è£…"
      ;;
    unhealthy)
      log "âŒ æ£€æµ‹åˆ° AWS Load Balancer Controller çŠ¶æ€å¼‚å¸¸, åˆ é™¤æ—§ Pod åé‡æ–°éƒ¨ç½²"
      kubectl -n $KUBE_DEFAULT_NAMESPACE delete pod -l $POD_ALBC_LABEL --ignore-not-found
      ;;
    *)
      log "âš ï¸  æœªçŸ¥çš„ AWS Load Balancer Controller çŠ¶æ€, ç»§ç»­å°è¯•å®‰è£…"
      ;;
  esac

  if ! helm repo list | grep -q "^${ALBC_HELM_REPO_NAME}\b"; then
    log "ğŸ”§ æ·»åŠ  ${ALBC_HELM_REPO_NAME} Helm ä»“åº“"
    helm repo add ${ALBC_HELM_REPO_NAME} ${ALBC_HELM_REPO_URL}
  fi
  helm repo update

  log "ğŸ“¦ åº”ç”¨ AWS Load Balancer Controller CRDs (version ${ALBC_CHART_VERSION})"
  tmp_dir=$(mktemp -d)
  helm pull ${ALBC_HELM_REPO_NAME}/${ALBC_CHART_NAME} --version ${ALBC_CHART_VERSION} --untar -d "$tmp_dir"
  kubectl apply -f "$tmp_dir/${ALBC_CHART_NAME}/crds"
  rm -rf "$tmp_dir"

  VPC_ID=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$REGION" --profile "$PROFILE" --query "cluster.resourcesVpcConfig.vpcId" --output text)

  log "ğŸš€ æ­£åœ¨é€šè¿‡ Helm å®‰è£…æˆ–å‡çº§ AWS Load Balancer Controller..."
  helm upgrade --install ${ALBC_RELEASE_NAME} ${ALBC_HELM_REPO_NAME}/${ALBC_CHART_NAME} \
    -n $KUBE_DEFAULT_NAMESPACE \
    --version ${ALBC_CHART_VERSION} \
    --set clusterName=$CLUSTER_NAME \
    --set region=$REGION \
    --set vpcId=$VPC_ID \
    --set serviceAccount.create=false \
    --set serviceAccount.name=${ALBC_SERVICE_ACCOUNT_NAME} \
    --set image.repository=${ALBC_IMAGE_REPO} \
    --set image.tag=${ALBC_IMAGE_TAG}

  log "ğŸ” ç­‰å¾… AWS Load Balancer Controller å°±ç»ª"
  kubectl -n $KUBE_DEFAULT_NAMESPACE rollout status deployment/${ALBC_RELEASE_NAME} --timeout=180s
  kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_ALBC_LABEL
}

# å®‰è£…æˆ–å‡çº§ Cluster Autoscaler
install_autoscaler() {
  local status
  status=$(check_autoscaler_status)
  case "$status" in
    healthy)
      log "âœ… Cluster Autoscaler å·²æ­£å¸¸è¿è¡Œ, è·³è¿‡ Helm éƒ¨ç½²"
      return 0
      ;;
    missing)
      log "âš™ï¸  æ£€æµ‹åˆ° Cluster Autoscaler æœªéƒ¨ç½², å¼€å§‹å®‰è£…"
      ;;
    unhealthy)
      log "âŒ æ£€æµ‹åˆ° Cluster Autoscaler çŠ¶æ€å¼‚å¸¸, åˆ é™¤æ—§ Pod åé‡æ–°éƒ¨ç½²"
      kubectl -n $KUBE_DEFAULT_NAMESPACE delete pod -l $POD_AUTOSCALER_LABEL --ignore-not-found
      ;;
    *)
      log "âš ï¸  æœªçŸ¥çš„ Cluster Autoscaler çŠ¶æ€, ç»§ç»­å°è¯•å®‰è£…"
      ;;
  esac
  log "ğŸš€ æ­£åœ¨é€šè¿‡ Helm å®‰è£…æˆ–å‡çº§ Cluster Autoscaler..."
  if ! helm repo list | grep -q '^autoscaler'; then
    log "ğŸ”§ æ·»åŠ  autoscaler Helm ä»“åº“"
    helm repo add autoscaler https://kubernetes.github.io/autoscaler
  fi
  helm repo update
  # è·å– Kubernetes å®Œæ•´ç‰ˆæœ¬ (å¦‚ v1.33.1)
  K8S_FULL_VERSION=$(kubectl version -o json | jq -r '.serverVersion.gitVersion')
  # æå–ä¸»æ¬¡ç‰ˆæœ¬å· (å¦‚ 1.33)
  K8S_MINOR_VERSION=$(echo "$K8S_FULL_VERSION" | sed -E 's/^v([0-9]+\.[0-9]+)\..*$/\1/')
  # ç¡®å®š Cluster Autoscaler ç‰ˆæœ¬ (æ€»æ˜¯ä½¿ç”¨ .0 è¡¥ä¸ç‰ˆæœ¬)
  AUTOSCALER_VERSION="v${K8S_MINOR_VERSION}.0"
  helm upgrade --install ${AUTOSCALER_RELEASE_NAME} autoscaler/${AUTOSCALER_CHART_NAME} -n $KUBE_DEFAULT_NAMESPACE --create-namespace \
    --set awsRegion=$REGION \
    --set autoDiscovery.clusterName=$CLUSTER_NAME \
    --set rbac.serviceAccount.create=true \
    --set rbac.serviceAccount.name=${AUTOSCALER_RELEASE_NAME} \
    --set extraArgs.balance-similar-node-groups=true \
    --set extraArgs.skip-nodes-with-system-pods=false \
    --set rbac.serviceAccount.annotations."eks\\.amazonaws\\.com/role-arn"="$AUTOSCALER_ROLE_ARN" \
    --set image.tag=$AUTOSCALER_VERSION
  log "âœ… Helm install completed"
  log "ğŸ” æ£€æŸ¥ Cluster Autoscaler Pod çŠ¶æ€"
  kubectl -n $KUBE_DEFAULT_NAMESPACE rollout status deployment/${DEPLOYMENT_AUTOSCALER_NAME} --timeout=180s
  kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_AUTOSCALER_LABEL
  log "å¦‚æœ Helm éƒ¨ç½²å¤±è´¥ï¼Œé‡æ–°éƒ¨ç½²åï¼Œéœ€è¦æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤åˆ é™¤æ—§ Pod è®© Deployment æ‹‰æ–°é…ç½®: "
  log "kubectl -n $KUBE_DEFAULT_NAMESPACE delete pod -l $POD_AUTOSCALER_LABEL"
}

# è·å–å½“å‰æœ€æ–° ASG å
get_latest_asg() {
  aws autoscaling describe-auto-scaling-groups \
    --region "$REGION" --profile "$PROFILE" \
    --query "AutoScalingGroups[?starts_with(AutoScalingGroupName, '$ASG_PREFIX')].AutoScalingGroupName" \
    --output text | head -n1
}

# ç»‘å®š SNS é€šçŸ¥
bind_sns_notification() {
  local asg_name="$1"
  log "ğŸ”„ ç»‘å®š SNS é€šçŸ¥åˆ° ASG: $asg_name"
  aws autoscaling put-notification-configuration \
    --auto-scaling-group-name "$asg_name" \
    --topic-arn "$TOPIC_ARN" \
    --notification-types "autoscaling:EC2_INSTANCE_TERMINATE" \
    --region "$REGION" --profile "$PROFILE"
}

# ç¡®ä¿ SNS ç»‘å®šåˆ°æœ€æ–° ASG
ensure_sns_binding() {
  local asg_name="$1"
  if [[ -f "$STATE_FILE" ]]; then
    last_bound_asg=$(cat "$STATE_FILE")
  else
    last_bound_asg=""
  fi
  if [[ "$asg_name" == "$last_bound_asg" ]]; then
    log "âœ… å½“å‰ ASG [$asg_name] å·²ç»‘å®šè¿‡, æ— éœ€é‡å¤ç»‘å®š"
  else
    bind_sns_notification "$asg_name"
    echo "$asg_name" > "$STATE_FILE"
    log "âœ… å·²ç»‘å®šå¹¶è®°å½•æœ€æ–° ASG: $asg_name"
  fi
}

# æ£€æŸ¥ NAT ç½‘å…³çŠ¶æ€
check_nat_gateway() {
  aws ec2 describe-nat-gateways \
    --region "$REGION" --profile "$PROFILE" \
    --query "NatGateways[?State=='available']" --output json | jq length
}

# æ£€æŸ¥ ALB çŠ¶æ€
check_alb() {
  aws elbv2 describe-load-balancers \
    --region "$REGION" --profile "$PROFILE" \
    --query "LoadBalancers[?Type=='application']" --output json | jq length
}

# æ£€æŸ¥ EKS é›†ç¾¤çŠ¶æ€
check_eks_cluster() {
  aws eks describe-cluster \
    --region "$REGION" --profile "$PROFILE" \
    --name "$CLUSTER_NAME" \
    --query 'cluster.status' --output text
}

# æ£€æŸ¥èŠ‚ç‚¹ç»„çŠ¶æ€
check_nodegroup() {
  aws eks describe-nodegroup \
    --region "$REGION" --profile "$PROFILE" \
    --cluster-name "$CLUSTER_NAME" \
    --nodegroup-name "$NODEGROUP_NAME" \
    --query 'nodegroup.status' --output text
}

# æ£€æŸ¥æ—¥å¿—ç»„å­˜åœ¨
check_log_group() {
  aws logs describe-log-groups \
    --region "$REGION" --profile "$PROFILE" \
    --log-group-name-prefix "/aws/eks/${CLUSTER_NAME}/cluster" \
    --query 'logGroups[*].logGroupName' --output text
}

# æ£€æŸ¥ SNS ç»‘å®š
check_sns_binding() {
  local asg_name="$1"
  aws autoscaling describe-auto-scaling-groups \
    --region "$REGION" --profile "$PROFILE" \
    --auto-scaling-group-names "$asg_name" \
    --query "AutoScalingGroups[0].NotificationConfigurations[?TopicARN=='${TOPIC_ARN}']" \
    --output json | jq length
}

# è¿›è¡ŒåŸºç¡€èµ„æºæ£€æŸ¥
perform_health_checks() {
  local asg_name="$1"
  log "ğŸ” å¼€å§‹æ‰§è¡ŒåŸºç¡€èµ„æºå¥åº·æ£€æŸ¥..."
  log "ğŸ” æ£€æŸ¥ NAT ç½‘å…³çŠ¶æ€"
  nat_count=$(check_nat_gateway)
  log "NAT Gateway count: $nat_count"
  log "ğŸ” æ£€æŸ¥ ALB çŠ¶æ€"
  alb_count=$(check_alb)
  log "ALB count: $alb_count"
  log "ğŸ” æ£€æŸ¥ EKS é›†ç¾¤çŠ¶æ€"
  eks_status=$(check_eks_cluster)
  log "EKS cluster status: $eks_status"
  log "ğŸ” æ£€æŸ¥èŠ‚ç‚¹ç»„çŠ¶æ€"
  node_status=$(check_nodegroup)
  log "NodeGroup status: $node_status"
  log "ğŸ” æ£€æŸ¥ LogGroup æ˜¯å¦å­˜åœ¨"
  log_group=$(check_log_group)
  log "LogGroup: $log_group"
  log "ğŸ” æ£€æŸ¥ AWS Load Balancer Controller éƒ¨ç½²çŠ¶æ€"
  albc_status=$(check_albc_status)
  log "AWS Load Balancer Controller status: $albc_status"
  log "ğŸ” æ£€æŸ¥ Cluster Autoscaler éƒ¨ç½²çŠ¶æ€"
  autoscaler_status=$(check_autoscaler_status)
  log "Cluster Autoscaler status: $autoscaler_status"
  log "ğŸ” éªŒè¯ SNS é€šçŸ¥ç»‘å®š"
  sns_bound=$(check_sns_binding "$asg_name")
  log "SNS bindings for ASG [$asg_name]: $sns_bound"
}

# === éƒ¨ç½² task-api åˆ° EKSï¼ˆå¹‚ç­‰ï¼‰===
deploy_task_api() {
  # ===== å‰ç½®ï¼šAWS èº«ä»½ä¸ kubeconfig =====
  log "ğŸ” ä½¿ç”¨ profile=${PROFILE} region=${REGION}"
  ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text --profile "${PROFILE}")" || abort "æ— æ³•è·å– AWS è´¦å· ID"
  log "ğŸ‘¤ AWS Account: ${ACCOUNT_ID}"

  log "ğŸ”§ é…ç½® kubeconfigï¼ˆcluster=${CLUSTER_NAME}ï¼‰"
  aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${REGION}" --profile "${PROFILE}" >/dev/null

  # ===== åº”ç”¨ Kubernetes æ¸…å• =====
  if [[ ! -d "${K8S_BASE_DIR}" ]]; then
    abort "æœªæ‰¾åˆ° k8s æ¸…å•ç›®å½•ï¼š${K8S_BASE_DIR}"
  fi
  log "ğŸ—‚ï¸  apply æ¸…å•ï¼šns-sa.yaml"
  kubectl -n "${NS}" apply -f "${K8S_BASE_DIR}/ns-sa.yaml"
  log "ğŸ—‚ï¸  apply æ¸…å•ï¼šconfigmap.yaml"
  kubectl -n "${NS}" apply -f "${K8S_BASE_DIR}/configmap.yaml"
  log "ğŸ—‚ï¸  apply æ¸…å•ï¼šdeploy-svc.yaml"
  kubectl -n "${NS}" apply -f "${K8S_BASE_DIR}/deploy-svc.yaml"

  # ===== è§£æé•œåƒï¼ˆä¼˜å…ˆä½¿ç”¨å›ºå®š digestï¼‰=====
  if [[ -n "${IMAGE_DIGEST:-}" ]]; then
    DIGEST="${IMAGE_DIGEST}"
    log "ğŸ“Œ ä½¿ç”¨å›ºå®š digestï¼š${DIGEST}"
  else
    log "ğŸ” ä» ECR è·å– ${ECR_REPO}:${IMAGE_TAG} çš„ digest"
    set +e
    DIGEST="$(aws ecr describe-images \
      --repository-name "${ECR_REPO}" \
      --image-ids imageTag="${IMAGE_TAG}" \
      --query 'imageDetails[0].imageDigest' \
      --output text --region "${REGION}" --profile "${PROFILE}")"
    rc=$?
    set -e
    if [[ $rc -ne 0 || -z "${DIGEST}" || "${DIGEST}" == "None" ]]; then
      abort "ECR ä¸­æœªæ‰¾åˆ°é•œåƒ ${ECR_REPO}:${IMAGE_TAG} çš„ digestï¼Œè¯·å…ˆæ¨é€é•œåƒæˆ–è°ƒæ•´ IMAGE_TAG"
    fi
  fi
  IMAGE="${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${ECR_REPO}@${DIGEST}"
  log "ğŸ–¼ï¸  å°†éƒ¨ç½²é•œåƒï¼š${IMAGE}"

  # ===== ç”¨ set image è¦†ç›–é•œåƒï¼Œå¹¶è®°å½• rollout å†å² =====
  log "â™»ï¸  æ›´æ–° Deployment é•œåƒå¹¶ç­‰å¾…æ»šåŠ¨å®Œæˆ"
  kubectl -n "${NS}" set image deploy/"${APP}" "${APP}"="${IMAGE}" --record
  kubectl -n "${NS}" rollout status deploy/"${APP}" --timeout=180s
  kubectl -n "${NS}" get deploy,svc -o wide

  # ===== é›†ç¾¤å†…å†’çƒŸæµ‹è¯• =====
  log "ğŸ§ª é›†ç¾¤å†…å†’çƒŸæµ‹è¯•ï¼š/api/hello ä¸ /actuator/health"
  kubectl -n "${NS}" run curl --image=curlimages/curl:8.8.0 -i --rm -q --restart=Never -- \
    sh -lc "set -e; \
      curl -sf http://${APP}.${NS}.svc.cluster.local:8080/api/hello?name=Renda >/dev/null; \
      curl -sf http://${APP}.${NS}.svc.cluster.local:8080/actuator/health | grep -q '\"status\":\"UP\"'"
  log "âœ… éƒ¨ç½²ä¸å†’çƒŸæµ‹è¯•å®Œæˆ"
}

# éƒ¨ç½² taskapi ingress
deploy_taskapi_ingress() {
  set -euo pipefail
  local outdir="${SCRIPT_DIR}/.out"; mkdir -p "$outdir"

  log "ğŸ“¦ Apply Ingress (${APP}) ..."
  # è‹¥æ— å˜æ›´å°±ä¸ applyï¼ˆ0=æ— å·®å¼‚ï¼Œ1=æœ‰å·®å¼‚ï¼Œ>1=å‡ºé”™ï¼‰
  if kubectl -n "$NS" diff -f "$ING_FILE" >/dev/null 2>&1; then
    log "â‰¡ No changes"
  else
    kubectl apply -f "$ING_FILE"
  fi

  # å¦‚æœå·²ç»æœ‰ ALBï¼Œå°±ç›´æ¥å¤ç”¨å¹¶è¿”å›
  local dns
  dns=$(kubectl -n "$NS" get ing "$APP" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
  if [[ -n "${dns}" ]]; then
    log "âœ… ALB ready: http://${dns}"
    echo "${dns}" > "${outdir}/alb_${APP}_dns"
    return 0
  fi

  log "â³ Waiting for ALB to be provisioned ..."
  local t=0; local timeout=600
  while [[ $t -lt $timeout ]]; do
    dns=$(kubectl -n "$NS" get ing "$APP" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
    [[ -n "${dns}" ]] && break
    sleep 5; t=$((t+5))
  done
  [[ -z "${dns}" ]] && { log "âŒ Timeout waiting ALB"; return 1; }

  log "âœ… ALB ready: http://${dns}"
  echo "${dns}" > "${outdir}/alb_${APP}_dns"

  log "ğŸ§ª Smoke"
  curl -s "http://${dns}/api/hello?name=Renda" | sed -n '1p'
  curl -s "http://${dns}/actuator/health" | sed -n '1p'
}

### ---- metrics-server (Helm) ----
deploy_metrics_server() {
  log "ğŸ“¦ Installing metrics-server ..."
  helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ >/dev/null 2>&1 || true
  helm repo update >/dev/null
  helm upgrade --install metrics-server metrics-server/metrics-server \
    --namespace $KUBE_DEFAULT_NAMESPACE \
    --version 3.12.1 \
    --set args={--kubelet-insecure-tls}
  kubectl -n $KUBE_DEFAULT_NAMESPACE rollout status deploy/metrics-server --timeout=180s
}

### ---- HPA for task-api ----
deploy_taskapi_hpa() {
  log "ğŸ“¦ Apply HPA for task-api ..."
  kubectl apply -f "$HPA_FILE"
  log "ğŸ” Describe HPA"
  kubectl -n svc-task describe hpa task-api | sed -n '1,60p' || true
}

# === ä¸»æµç¨‹ ===
log "ğŸ“£ å¼€å§‹æ‰§è¡Œ post-recreate è„šæœ¬"

if ! cluster_exists; then
  log "âš ï¸  æœªæ‰¾åˆ° EKS é›†ç¾¤ $CLUSTER_NAMEï¼Œå¯èƒ½å·²é”€æ¯ï¼Œè„šæœ¬é€€å‡º"
  exit 0
fi

log "ğŸ” è·å–æœ€æ–°çš„ ASG åç§°"
asg_name=$(get_latest_asg)
if [[ -z "$asg_name" ]]; then
  log "âŒ æœªæ‰¾åˆ°ä»¥ $ASG_PREFIX å¼€å¤´çš„ ASG, ç»ˆæ­¢è„šæœ¬"
  exit 1
fi

update_kubeconfig

install_albc_controller

install_autoscaler

ensure_sns_binding "$asg_name"

perform_health_checks "$asg_name"

deploy_task_api

deploy_taskapi_ingress

deploy_metrics_server

deploy_taskapi_hpa
