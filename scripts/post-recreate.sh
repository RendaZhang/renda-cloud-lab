#!/usr/bin/env bash
# ------------------------------------------------------------
# Renda Cloud Lab Â· post-recreate.sh
# éœ€è¦ä½¿ç”¨ Terraform æˆåŠŸå¯åŠ¨äº†åŸºç¡€è®¾æ–½ï¼ˆNAT + ALB + EKS + IRSAï¼‰åï¼Œ
# å†ä½¿ç”¨æœ¬è„šæœ¬è¿›è¡Œéƒ¨ç½²å±‚çš„è‡ªåŠ¨åŒ–æ“ä½œã€‚
# ç¡®ä¿å°†é›†ç¾¤èµ„æºçš„åˆ›å»ºä¸ Kubernetes æœåŠ¡çš„éƒ¨ç½²è¿›è¡Œè§£è€¦ã€‚
#
# å¿…éœ€çš„ç¯å¢ƒå˜é‡ï¼ˆéœ€åœ¨è¿è¡Œå‰è®¾ç½®æˆ–ç”±é›†ç¾¤è‡ªåŠ¨æ³¨å…¥ï¼‰ï¼š
# å¦‚ä¸‹ä¸‰ä¸ªè‡ªå®šä¹‰å˜é‡éœ€è¦åœ¨ ${ROOT_DIR}/task-api/k8s/base/configmap.yaml ä¸­å®šä¹‰
#   S3_BUCKET
#   S3_PREFIX
#   AWS_REGION
# å¦‚ä¸‹ä¸¤ä¸ªä¼šç”± EKS è‡ªåŠ¨æ³¨å…¥
#   AWS_ROLE_ARN
#   AWS_WEB_IDENTITY_TOKEN_FILE
# "å¦‚æœ Helm éƒ¨ç½²å¤±è´¥ï¼Œé‡æ–°éƒ¨ç½²åï¼Œéœ€è¦æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤åˆ é™¤æ—§ Pod è®© Deployment æ‹‰æ–°é…ç½®: "
# log "kubectl -n $KUBE_DEFAULT_NAMESPACE delete pod -l $POD_AUTOSCALER_LABEL"
#
# åŠŸèƒ½ï¼š
#   1. æ›´æ–°æœ¬åœ° kubeconfig å¹¶ç­‰å¾…é›†ç¾¤ API å°±ç»ª
#   2. åˆ›å»º/æ›´æ–° AWS Load Balancer Controller æ‰€éœ€çš„ ServiceAccountï¼ˆIRSAï¼‰
#   3. ç¡®ä¿ task-api çš„ ServiceAccount å­˜åœ¨å¹¶å¸¦ IRSA æ³¨è§£
#   4. é€šè¿‡ Helm å®‰è£…æˆ–å‡çº§ AWS Load Balancer Controller
#   5. é€šè¿‡ Helm å®‰è£…æˆ–å‡çº§ ${AUTOSCALER_RELEASE_NAME}
#   6. æ£€æŸ¥ NAT ç½‘å…³ã€ALBã€EKS æ§åˆ¶é¢å’ŒèŠ‚ç‚¹ç»„ç­‰çŠ¶æ€
#   7. è·å–æœ€æ–°çš„ EKS NodeGroup ç”Ÿæˆçš„ ASG åç§°
#   8. è‹¥ä¹‹å‰æœªç»‘å®šï¼Œåˆ™ä¸ºè¯¥ ASG é…ç½® SNS Spot Interruption é€šçŸ¥
#   9. è‡ªåŠ¨å†™å…¥ç»‘å®šæ—¥å¿—ï¼Œé¿å…é‡å¤æ‰§è¡Œ
#  10. éƒ¨ç½² task-apiï¼ˆé•œåƒç”± task-api å­é¡¹ç›®æ„å»ºå¹¶å›ºå®š ECR digestï¼Œé…ç½®æ¢é’ˆ/èµ„æºï¼Œå¹¶åˆ›å»º PodDisruptionBudgetï¼‰å¹¶åœ¨é›†ç¾¤å†…å†’çƒŸ
#  11. å‘å¸ƒ Ingressï¼Œç­‰å¾…å…¬ç½‘ ALB å°±ç»ªå¹¶åš HTTP å†’çƒŸ
#  12. å®‰è£… metrics-serverï¼ˆ--kubelet-insecure-tlsï¼‰
#  13. å®‰è£…/å‡çº§ ADOT Collector å¹¶é…ç½®å‘ AMP å†™æŒ‡æ ‡ï¼ˆIRSA + SigV4ï¼‰
#  14. å®‰è£…/å‡çº§ Grafanaï¼ˆIRSA + SigV4 æ’ä»¶ï¼‰
#  15. ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸å¼€å¯ï¼‰å®‰è£… Chaos Meshï¼ˆä»… controller + daemonsetï¼‰
#  16. éƒ¨ç½² HPAï¼ˆCPU 60%ï¼Œmin=2/max=10ï¼Œå« behaviorï¼‰
#  17. æ£€æŸ¥ task-api
# ä½¿ç”¨ï¼š
#   bash scripts/post-recreate.sh
# ------------------------------------------------------------

set -euo pipefail

# === å¯é…ç½®å‚æ•°ï¼Œå¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›– ===
CLOUD_PROVIDER="${CLOUD_PROVIDER:-aws}"
PROFILE=${AWS_PROFILE:-phase2-sso}
REGION=${REGION:-us-east-1}
AWS_PROFILE=${PROFILE}
AWS_REGION=${REGION}
ACCOUNT_ID=${ACCOUNT_ID:-$(aws sts get-caller-identity --query Account --profile "$PROFILE" --output text)}
echo "ä½¿ç”¨ AWS è´¦å·: $ACCOUNT_ID"

CLUSTER_NAME="${CLUSTER_NAME:-dev}"
NODEGROUP_NAME="${NODEGROUP_NAME:-ng-mixed}"
KUBE_DEFAULT_NAMESPACE="${KUBE_DEFAULT_NAMESPACE:-kube-system}"
ASG_PREFIX="${ASG_PREFIX:-eks-${NODEGROUP_NAME}}"

# === åº”ç”¨éƒ¨ç½²å‚æ•°ï¼ˆå¯è¢«ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰===
# k8s å‘½åç©ºé—´ï¼ˆéœ€ä¸æ¸…å•ä¸­çš„ metadata.namespace ä¸€è‡´ï¼‰
NS="${NS:-svc-task}"
# Deployment/Service çš„åç§°ä¸å®¹å™¨å
APP="${APP:-task-api}"
# PodDisruptionBudget åç§°ï¼ˆä¸ Deployment åŒå + "-pdb"ï¼‰
PDB_NAME="${PDB_NAME:-${APP}-pdb}"
# ECR ä»“åº“å
ECR_REPO="${ECR_REPO:-task-api}"
# IRSA è§’è‰²åç§°ä¸ ARNï¼ˆåº”ç”¨çº§ ServiceAccount ä½¿ç”¨ï¼‰
TASK_API_ROLE_NAME="${TASK_API_ROLE_NAME:-dev-task-api-irsa}"
TASK_API_ROLE_ARN="${TASK_API_ROLE_ARN:-arn:${CLOUD_PROVIDER}:iam::${ACCOUNT_ID}:role/${TASK_API_ROLE_NAME}}"
TASK_API_SERVICE_ACCOUNT_NAME="${TASK_API_SERVICE_ACCOUNT_NAME:-${APP}}"
# è¦éƒ¨ç½²çš„ task-api é•œåƒ tagï¼ˆä¹Ÿå¯ç”¨ latestï¼‰ã€‚è‹¥è®¾ç½® IMAGE_DIGEST åˆ™ä¼˜å…ˆç”Ÿæ•ˆã€‚
# å¦‚æ›´æ–° task-api æºç ï¼Œè¯·å…ˆæ„å»ºå¹¶æ¨é€æ–°é•œåƒï¼Œç„¶åè°ƒæ•´æ­¤å¤„ tag æˆ–è®¾ç½® IMAGE_DIGESTã€‚
IMAGE_TAG="${IMAGE_TAG:-0.1.0-2508272044}"
# k8s æ¸…å•æ‰€åœ¨ç›®å½•ï¼ˆns-sa.yaml / configmap.yaml / deploy-svc.yaml / pdb.yamlï¼‰
SCRIPT_DIR="${SCRIPT_DIR:-$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)}"
ROOT_DIR="${ROOT_DIR:-$(cd "${SCRIPT_DIR}/.." && pwd)}"
K8S_BASE_DIR="${K8S_BASE_DIR:-${ROOT_DIR}/task-api/k8s/base}"
# è‹¥æƒ³å›ºå®šæŸä¸ª digestï¼Œå¯åœ¨è¿è¡Œå‰ export IMAGE_DIGEST=sha256:...

# ä¸º ASG é…ç½® Spot Interruption é€šçŸ¥çš„å‚æ•°
TOPIC_NAME="${TOPIC_NAME:-spot-interruption-topic}"
TOPIC_ARN="${TOPIC_ARN:-arn:${CLOUD_PROVIDER}:sns:${REGION}:${ACCOUNT_ID}:${TOPIC_NAME}}"
STATE_FILE="${STATE_FILE:-${SCRIPT_DIR}/.last-asg-bound}"
# ASG ç›¸å…³å‚æ•°
AUTOSCALER_CHART_NAME="${AUTOSCALER_CHART_NAME:-cluster-autoscaler}"
AUTOSCALER_RELEASE_NAME=${AUTOSCALER_CHART_NAME}
AUTOSCALER_HELM_REPO_NAME="${AUTOSCALER_HELM_REPO_NAME:-autoscaler}"
AUTOSCALER_HELM_REPO_URL="${AUTOSCALER_HELM_REPO_URL:-https://kubernetes.github.io/autoscaler}"
AUTOSCALER_SERVICE_ACCOUNT_NAME=${AUTOSCALER_CHART_NAME}
AUTOSCALER_ROLE_NAME="${AUTOSCALER_ROLE_NAME:-eks-cluster-autoscaler}"
AUTOSCALER_ROLE_ARN="${AUTOSCALER_ROLE_ARN:-arn:${CLOUD_PROVIDER}:iam::${ACCOUNT_ID}:role/${AUTOSCALER_ROLE_NAME}}"
AUTOSCALER_DEPLOYMENT_NAME="${AUTOSCALER_DEPLOYMENT_NAME:-${AUTOSCALER_RELEASE_NAME}-${CLOUD_PROVIDER}-${AUTOSCALER_CHART_NAME}}"
POD_AUTOSCALER_LABEL="${POD_AUTOSCALER_LABEL:-app.kubernetes.io/name=${AUTOSCALER_RELEASE_NAME}}"

# AWS Load Balancer Controller settings
ALBC_CHART_NAME="${ALBC_CHART_NAME:-aws-load-balancer-controller}"
ALBC_RELEASE_NAME=${ALBC_CHART_NAME}
ALBC_SERVICE_ACCOUNT_NAME=${ALBC_CHART_NAME}
ALBC_CHART_VERSION="${ALBC_CHART_VERSION:-1.8.1}"
ALBC_IMAGE_TAG="${ALBC_IMAGE_TAG:-v2.8.1}"
ALBC_IMAGE_REPO="${ALBC_IMAGE_REPO:-602401143452.dkr.ecr.${REGION}.amazonaws.com/amazon/aws-load-balancer-controller}"
ALBC_HELM_REPO_NAME="${ALBC_HELM_REPO_NAME:-eks}"
ALBC_HELM_REPO_URL="${ALBC_HELM_REPO_URL:-https://aws.github.io/eks-charts}"
POD_ALBC_LABEL="${POD_ALBC_LABEL:-app.kubernetes.io/name=${ALBC_RELEASE_NAME}}"
ALBC_ROLE_NAME="${ALBC_ROLE_NAME:-aws-load-balancer-controller}"
ALBC_ROLE_ARN="${ALBC_ROLE_ARN:-arn:${CLOUD_PROVIDER}:iam::${ACCOUNT_ID}:role/${ALBC_ROLE_NAME}}"
# ADOT Collector + AMP settings
ADOT_NAMESPACE="${ADOT_NAMESPACE:-observability}"
ADOT_RELEASE_NAME="${ADOT_RELEASE_NAME:-adot-collector}"
ADOT_SERVICE_ACCOUNT_NAME="${ADOT_SERVICE_ACCOUNT_NAME:-adot-collector}"
ADOT_DEPLOYMENT_NAME="${ADOT_DEPLOYMENT_NAME:-${ADOT_RELEASE_NAME}-opentelemetry-collector}"
ADOT_HELM_REPO_NAME="${ADOT_HELM_REPO_NAME:-open-telemetry}"
ADOT_HELM_REPO_URL="${ADOT_HELM_REPO_URL:-https://open-telemetry.github.io/opentelemetry-helm-charts}"
# IRSA è§’è‰²ï¼ˆé»˜è®¤ä½¿ç”¨å½“å‰è´¦å·ä¸‹çš„ adot-collector è§’è‰²åï¼›å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
ADOT_ROLE_NAME="${ADOT_ROLE_NAME:-adot-collector}"
ADOT_ROLE_ARN="${ADOT_ROLE_ARN:-arn:${CLOUD_PROVIDER}:iam::${ACCOUNT_ID}:role/${ADOT_ROLE_NAME}}"
# Helm values æ–‡ä»¶è·¯å¾„ï¼ˆå›ºå®šåœ¨ task-api/k8s ä¸‹ï¼Œä¾¿äºå®¡é˜…ä¸ç‰ˆæœ¬æ§åˆ¶ï¼‰
ADOT_VALUES_FILE="${ADOT_VALUES_FILE:-${ROOT_DIR}/task-api/k8s/adot-collector-values.yaml}"

# Grafana settings
GRAFANA_NAMESPACE="${GRAFANA_NAMESPACE:-observability}"
GRAFANA_RELEASE_NAME="${GRAFANA_RELEASE_NAME:-grafana}"
GRAFANA_HELM_REPO_NAME="${GRAFANA_HELM_REPO_NAME:-grafana}"
GRAFANA_HELM_REPO_URL="${GRAFANA_HELM_REPO_URL:-https://grafana.github.io/helm-charts}"
GRAFANA_VALUES_FILE="${GRAFANA_VALUES_FILE:-${ROOT_DIR}/task-api/k8s/grafana-values.yaml}"
GRAFANA_SERVICE_ACCOUNT_NAME="${GRAFANA_SERVICE_ACCOUNT_NAME:-grafana}"
GRAFANA_ROLE_NAME="${GRAFANA_ROLE_NAME:-grafana-amp-query}"
GRAFANA_ROLE_ARN="${GRAFANA_ROLE_ARN:-arn:${CLOUD_PROVIDER}:iam::${ACCOUNT_ID}:role/${GRAFANA_ROLE_NAME}}"

# Chaos Mesh settingsï¼ˆå¯é€‰å®‰è£…ï¼‰
# ENABLE_CHAOS_MESH=true åˆ™å®‰è£… Chaos Mesh
ENABLE_CHAOS_MESH="${ENABLE_CHAOS_MESH:-false}"
CHAOS_NAMESPACE="${CHAOS_NAMESPACE:-chaos-testing}"
CHAOS_RELEASE_NAME="${CHAOS_RELEASE_NAME:-chaos-mesh}"
CHAOS_HELM_REPO_NAME="${CHAOS_HELM_REPO_NAME:-chaos-mesh}"
CHAOS_DEPLOYMENT_NAME="${CHAOS_DEPLOYMENT_NAME:-${CHAOS_RELEASE_NAME}-controller-manager}"
CHAOS_HELM_REPO_URL="${CHAOS_HELM_REPO_URL:-https://charts.chaos-mesh.org}"
CHAOS_VALUES_FILE="${CHAOS_VALUES_FILE:-${ROOT_DIR}/task-api/k8s/chaos-mesh-values.yaml}"

# ---- Ingress ----
ING_FILE="${ING_FILE:-${ROOT_DIR}/task-api/k8s/ingress.yaml}"
# ---- HPA ----
HPA_FILE="${HPA_FILE:-${ROOT_DIR}/task-api/k8s/hpa.yaml}"
# ---- In-cluster Smoke Test ----
SMOKE_FILE="${SMOKE_FILE:-${ROOT_DIR}/task-api/k8s/task-api-smoke.yaml}"

# === å‡½æ•°å®šä¹‰ ===
# æ¸…ç†ä¸´æ—¶ Job/èµ„æºï¼Œé¿å…è„šæœ¬å¼‚å¸¸é€€å‡ºåæ®‹ç•™
cleanup() {
  kubectl -n "$NS" delete job task-api-smoke awscli-smoke --ignore-not-found >/dev/null 2>&1 || true
}
trap cleanup EXIT ERR
# æ—¥å¿—ä¸é”™è¯¯å¤„ç†
log() {
  printf "[%s] %s\n" "$(date '+%H:%M:%S')" "$*";
}
abort() {
  printf "[%s] âŒ %s\n" "$(date '+%H:%M:%S')" "$*" >&2; exit 1;
}

# åˆ¤æ–­ EKS é›†ç¾¤æ˜¯å¦å­˜åœ¨
cluster_exists() {
  aws eks describe-cluster \
    --name "$CLUSTER_NAME" \
    --region "$REGION" \
    --profile "$PROFILE" >/dev/null 2>&1
}

# æ›´æ–° kubeconfig ä»¥è¿æ¥ EKS é›†ç¾¤
update_kubeconfig() {
  log "ğŸ”„ æ›´æ–° kubeconfig ä»¥è¿æ¥ EKS é›†ç¾¤: $CLUSTER_NAME"
  aws eks update-kubeconfig \
    --region "$REGION" \
    --name "$CLUSTER_NAME" \
    --profile "$PROFILE"
}

# ç­‰å¾…é›†ç¾¤ API Server å°±ç»ªï¼Œé¿å…åç»­ kubectl æ“ä½œè¶…æ—¶
wait_cluster_ready() {
  local timeout=180
  log "â³ ç­‰å¾… EKS é›†ç¾¤ API å°±ç»ª..."
  SECONDS=0
  until kubectl get nodes >/dev/null 2>&1; do
    if (( SECONDS >= timeout )); then
      abort "EKS é›†ç¾¤ API åœ¨ ${timeout}s å†…æœªå°±ç»ª"
    fi
    sleep 5
  done
  log "âœ… EKS é›†ç¾¤ API å·²å°±ç»ª"
}

# æ£€æŸ¥ Cluster Autoscaler éƒ¨ç½²çŠ¶æ€
check_autoscaler_status() {
  if ! kubectl -n $KUBE_DEFAULT_NAMESPACE get deployment $AUTOSCALER_DEPLOYMENT_NAME >/dev/null 2>&1; then
    echo "missing"
    return
  fi
  if kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_AUTOSCALER_LABEL \
      --no-headers 2>/dev/null | grep -v Running >/dev/null; then
    echo "unhealthy"
  else
    echo "healthy"
  fi
}

# æ£€æŸ¥ AWS Load Balancer Controller éƒ¨ç½²çŠ¶æ€
check_albc_status() {
  if ! kubectl -n $KUBE_DEFAULT_NAMESPACE get deployment $ALBC_RELEASE_NAME >/dev/null 2>&1; then
    echo "missing"
    return
  fi
  if kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_ALBC_LABEL \
      --no-headers 2>/dev/null | grep -v Running >/dev/null; then
    echo "unhealthy"
  else
    echo "healthy"
  fi
}

# ç¡®ä¿ AWS Load Balancer Controller çš„ ServiceAccount å­˜åœ¨å¹¶å¸¦æ³¨è§£
ensure_albc_service_account() {
  log "ğŸ› ï¸ ç¡®ä¿ ServiceAccount ${ALBC_SERVICE_ACCOUNT_NAME} å­˜åœ¨"
  if ! kubectl -n $KUBE_DEFAULT_NAMESPACE get sa ${ALBC_SERVICE_ACCOUNT_NAME} >/dev/null 2>&1; then
    kubectl -n $KUBE_DEFAULT_NAMESPACE create serviceaccount ${ALBC_SERVICE_ACCOUNT_NAME}
  fi
  kubectl -n $KUBE_DEFAULT_NAMESPACE annotate sa ${ALBC_SERVICE_ACCOUNT_NAME} \
    "eks.amazonaws.com/role-arn=${ALBC_ROLE_ARN}" --overwrite
}

# ç¡®ä¿ task-api çš„ ServiceAccount å­˜åœ¨å¹¶å¸¦ IRSA æ³¨è§£
ensure_task_api_service_account() {
  log "ğŸ› ï¸ ç¡®ä¿ task-api ServiceAccount ${TASK_API_SERVICE_ACCOUNT_NAME} å­˜åœ¨å¹¶å¸¦ IRSA æ³¨è§£"
  if ! kubectl -n $NS get sa $TASK_API_SERVICE_ACCOUNT_NAME >/dev/null 2>&1; then
    log "åˆ›å»º ServiceAccount ${TASK_API_SERVICE_ACCOUNT_NAME}"
    kubectl -n ${NS} create serviceaccount ${TASK_API_SERVICE_ACCOUNT_NAME}
  fi
  # å†™å…¥/è¦†ç›– IRSA æ³¨è§£
  kubectl -n ${NS} annotate sa ${TASK_API_SERVICE_ACCOUNT_NAME} \
    "eks.amazonaws.com/role-arn=${TASK_API_ROLE_ARN}" --overwrite
}

# ç¡®è®¤ Deployment æ»šåŠ¨æ›´æ–°å°±ç»ª
check_deployment_ready() {
  log "â³ ç­‰å¾… Deployment ${APP} å°±ç»ª"
  if ! kubectl -n "$NS" rollout status deploy/"${APP}" --timeout=180s; then
    abort "Deployment ${APP} æœªåœ¨ 180s å†…å°±ç»ª"
  fi
  log "âœ… Deployment ${APP} å·²å°±ç»ª"
}

# é›†ç¾¤å†…å†’çƒŸæµ‹è¯•
task_api_smoke_test() {
  log "ğŸ§ª é›†ç¾¤å†…å†’çƒŸæµ‹è¯•"
  kubectl -n "${NS}" apply -f "${SMOKE_FILE}"

  if ! kubectl -n "${NS}" wait --for=condition=complete job/task-api-smoke --timeout=60s; then
    kubectl -n "${NS}" logs job/task-api-smoke || true
    kubectl -n "${NS}" delete job task-api-smoke --ignore-not-found
    abort "é›†ç¾¤å†…å†’çƒŸæµ‹è¯•å¤±è´¥"
  fi
  kubectl -n "${NS}" logs job/task-api-smoke || true
  kubectl -n "${NS}" delete job task-api-smoke --ignore-not-found
  log "âœ… éƒ¨ç½²ä¸å†’çƒŸæµ‹è¯•å®Œæˆ"
}

# éªŒè¯ IRSA æ³¨å…¥ä¸è¿è¡Œæ—¶ç¯å¢ƒ
verify_irsa_env() {
  log "ğŸ” éªŒè¯ IRSA æ³¨å…¥ä¸è¿è¡Œæ—¶ç¯å¢ƒ"

  local summary=()
  local fails=0

  # --- task-api ServiceAccount annotation ---
  local sa_arn
  sa_arn=$(kubectl -n "${NS}" get sa "${TASK_API_SERVICE_ACCOUNT_NAME}" -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}' 2>/dev/null || true)
  if [[ "$sa_arn" == "$TASK_API_ROLE_ARN" ]]; then
    summary+=("âœ… task-api ServiceAccount æ³¨è§£æ­£ç¡®")
  else
    summary+=("âŒ task-api ServiceAccount æ³¨è§£ç¼ºå¤±æˆ–ä¸åŒ¹é… (got='${sa_arn}')")
    fails=$((fails+1))
  fi

  # --- task-api Pod checks ---
  local pod
  pod=$(kubectl -n "${NS}" get pods -l app="${TASK_API_SERVICE_ACCOUNT_NAME}" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || true)
  if [[ -z "$pod" ]]; then
    summary+=("âŒ æœªæ‰¾åˆ° ${APP} Podï¼Œæ— æ³•è¿›è¡Œ IRSA è‡ªæ£€")
    fails=$((fails+1))
  else
    local wait_time=0
    local max_wait=60
    local pod_status
    while [[ $wait_time -lt $max_wait ]]; do
      pod_status=$(kubectl -n "${NS}" get pod "$pod" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
      [[ "$pod_status" == "Running" ]] && break
      sleep 3
      wait_time=$((wait_time+3))
    done
    if [[ "$pod_status" != "Running" ]]; then
      summary+=("âŒ Pod $pod æœªè¿›å…¥ Running çŠ¶æ€ (å½“å‰: $pod_status)")
      fails=$((fails+1))
    else
      local env_out missing_env=()
      if ! env_out=$(kubectl -n "${NS}" exec "$pod" -- sh -lc 'env'); then
        summary+=("âŒ æ— æ³•è·å– Pod ç¯å¢ƒå˜é‡")
        fails=$((fails+1))
      else
        for key in S3_BUCKET S3_PREFIX AWS_REGION AWS_ROLE_ARN AWS_WEB_IDENTITY_TOKEN_FILE; do
          if ! echo "$env_out" | grep -q "^${key}="; then
            missing_env+=("$key")
          fi
        done
        if (( ${#missing_env[@]} > 0 )); then
          summary+=("âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: ${missing_env[*]}")
          fails=$((fails+1))
        else
          summary+=("âœ… ç¯å¢ƒå˜é‡æ³¨å…¥æ­£ç¡®")
        fi
      fi

      if kubectl -n "${NS}" exec "$pod" -- sh -lc 'ls -l /var/run/secrets/eks.amazonaws.com/serviceaccount/ && [ -s /var/run/secrets/eks.amazonaws.com/serviceaccount/token ]' >/dev/null; then
        summary+=("âœ… WebIdentity Token å­˜åœ¨ä¸”éç©º")
      else
        summary+=("âŒ WebIdentity Token ç¼ºå¤±æˆ–ä¸ºç©º")
        fails=$((fails+1))
      fi
    fi
  fi

  # --- ADOT Collector ServiceAccount annotation ---
  local adot_sa_arn
  adot_sa_arn=$(kubectl -n "$ADOT_NAMESPACE" get sa "$ADOT_SERVICE_ACCOUNT_NAME" -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}' 2>/dev/null || true)
  if [[ "$adot_sa_arn" == "$ADOT_ROLE_ARN" ]]; then
    summary+=("âœ… ADOT Collector ServiceAccount æ³¨è§£æ­£ç¡®")
  else
    summary+=("âŒ ADOT Collector ServiceAccount æ³¨è§£ç¼ºå¤±æˆ–ä¸åŒ¹é… (got='${adot_sa_arn}')")
    fails=$((fails+1))
  fi

  # --- Grafana ServiceAccount annotation ---
  local grafana_sa_arn
  grafana_sa_arn=$(kubectl -n "$GRAFANA_NAMESPACE" get sa "$GRAFANA_SERVICE_ACCOUNT_NAME" -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}' 2>/dev/null || true)
  if [[ "$grafana_sa_arn" == "$GRAFANA_ROLE_ARN" ]]; then
    summary+=("âœ… Grafana ServiceAccount æ³¨è§£æ­£ç¡®")
  else
    summary+=("âŒ Grafana ServiceAccount æ³¨è§£ç¼ºå¤±æˆ–ä¸åŒ¹é… (got='${grafana_sa_arn}')")
    fails=$((fails+1))
  fi

  for item in "${summary[@]}"; do
    log "$item"
  done

  [[ $fails -eq 0 ]]
}

# éªŒè¯ PodDisruptionBudget
check_pdb() {
  log "ğŸ” éªŒè¯ PodDisruptionBudget (${PDB_NAME})"

  kubectl -n "${NS}" get pdb "${PDB_NAME}" >/dev/null 2>&1 || \
    abort "ç¼ºå°‘ PodDisruptionBudget ${PDB_NAME}"

  local pdb_min disruptions_allowed current_healthy desired_healthy
  pdb_min=$(kubectl -n "${NS}" get pdb "${PDB_NAME}" -o jsonpath='{.spec.minAvailable}')
  disruptions_allowed=$(kubectl -n "${NS}" get pdb "${PDB_NAME}" -o jsonpath='{.status.disruptionsAllowed}')
  current_healthy=$(kubectl -n "${NS}" get pdb "${PDB_NAME}" -o jsonpath='{.status.currentHealthy}')
  desired_healthy=$(kubectl -n "${NS}" get pdb "${PDB_NAME}" -o jsonpath='{.status.desiredHealthy}')

  [[ "$pdb_min" != "1" ]] && abort "PodDisruptionBudget minAvailable=$pdb_min (expected 1)"
  disruptions_allowed=${disruptions_allowed:-0}
  current_healthy=${current_healthy:-0}
  desired_healthy=${desired_healthy:-0}

  if [ "$disruptions_allowed" -lt 1 ]; then
    abort "PodDisruptionBudget disruptionsAllowed=$disruptions_allowed (<1)ï¼Œå¯èƒ½æ˜¯å°±ç»ªå‰¯æœ¬ä¸è¶³æˆ–æ¢é’ˆæœª READY"
  fi

  if [ "$current_healthy" -lt "$desired_healthy" ]; then
    abort "PodDisruptionBudget currentHealthy=$current_healthy < desiredHealthy=$desired_healthy"
  fi

  log "âœ… PodDisruptionBudget æ£€æŸ¥é€šè¿‡ (allowed=${disruptions_allowed}, healthy=${current_healthy}/${desired_healthy})"
}

# éªŒè¯ ALB/Ingress/DNS
check_ingress_alb() {
  log "ğŸ” éªŒè¯ task-api ALBã€Ingressã€dns"

  local outdir="${SCRIPT_DIR}/.out"; mkdir -p "$outdir"
  local dns

  log "â³ Waiting for ALB to be provisioned ..."
  local t=0; local timeout=600
  while [[ $t -lt $timeout ]]; do
    dns=$(kubectl -n "$NS" get ing "$APP" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
    [[ -n "${dns}" ]] && break
    sleep 5; t=$((t+5))
  done
  [[ -z "${dns}" ]] && abort "Timeout waiting ALB"

  log "âœ… ALB ready: http://${dns}"
  echo "${dns}" > "${outdir}/alb_${APP}_dns"

  log "ğŸ§ª ALB DNS Smoke test: "
  local smoke_retries=10
  local smoke_ok=0
  local smoke_wait=5
  for ((i=1; i<=smoke_retries; i++)); do
    if curl -sf "http://${dns}/api/hello?name=Renda" | sed -n '1p'; then
      smoke_ok=1
      break
    else
      log "â³ Smoke test attempt $i/${smoke_retries} failed, retrying in ${smoke_wait}s..."
      sleep $smoke_wait
    fi
  done
  [[ $smoke_ok -eq 0 ]] && abort "Smoke test failed: /api/hello (DNS may not be ready or network issue)"
  curl -s "http://${dns}/actuator/health" | grep '"status":"UP"' || abort "Health check failed"
  curl -s "http://${dns}/actuator/prometheus" | grep '# HELP application_ready_time_seconds' || abort "Prometheus endpoint check failed"

  log "âœ… ALB DNS Smoke test passed"
}

# ---- aws-cli IRSA smoke test ----
# Launches a temporary aws-cli Job (serviceAccount=task-api) to:
#   1) call STS get-caller-identity
#   2) write/list/read under the allowed S3 prefix
#   3) verify writes to a disallowed prefix are denied
awscli_s3_smoke() {
  log "ğŸ§ª aws-cli IRSA S3 smoke test"
  local manifest="${ROOT_DIR}/task-api/k8s/awscli-smoke.yaml"

  kubectl -n "$NS" apply -f "$manifest"

  if ! kubectl -n "$NS" wait --for=condition=complete job/awscli-smoke --timeout=180s; then
    kubectl -n "$NS" logs job/awscli-smoke || true
    kubectl -n "$NS" delete job awscli-smoke --ignore-not-found
    abort "aws-cli smoke job failed"
  fi

  kubectl -n "$NS" logs job/awscli-smoke || true
  kubectl -n "$NS" delete job awscli-smoke --ignore-not-found
  log "âœ… aws-cli smoke test finished"
}

check_adot_ready() {
  log "ğŸ” ADOT Collector ç«¯åˆ°ç«¯éªŒè¯"
  local status
  status=$(check_adot_status)
  [[ "$status" != "healthy" ]] && abort "ADOT Collector çŠ¶æ€å¼‚å¸¸: $status"

  # ç«¯å£è½¬å‘åˆ° ADOT Collector çš„ Prometheus ç›‘å¬ç«¯å£
  kubectl -n "$ADOT_NAMESPACE" port-forward deploy/"${ADOT_DEPLOYMENT_NAME}" 8888 >/tmp/adot-pf.log 2>&1 &
  local pf_pid=$!

  local metric_value=""
  local retries=0
  while [[ $retries -lt 5 ]]; do
    metric_value=$(curl -s localhost:8888/metrics | grep 'otelcol_exporter_sent_metric_points{exporter="prometheusremotewrite"' | awk '{print $2}')
    [[ -n "$metric_value" ]] && break
    sleep 2
    retries=$((retries+1))
  done

  kill $pf_pid 2>/dev/null || true
  wait $pf_pid 2>/dev/null || true

  if [[ "$metric_value" =~ ^[0-9]+$ && "$metric_value" -gt 0 ]]; then
    log "âœ… ADOT Collector Remote Write å·²å‘é€ metric points: $metric_value"
  else
    abort "ADOT Collector Remote Write æœªç”Ÿæ•ˆ"
  fi
}

check_grafana_ready() {
  log "ğŸ” Grafana ç«¯åˆ°ç«¯éªŒè¯"
  local status
  status=$(check_grafana_status)
  [[ "$status" != "healthy" ]] && abort "Grafana çŠ¶æ€å¼‚å¸¸: $status"

  # ä¾æ¬¡å°è¯•å¤šä¸ªæœ¬åœ°ç«¯å£ï¼Œé¿å… 3000 è¢«å ç”¨å¯¼è‡´ç«¯å£è½¬å‘å¤±è´¥
  local ports=(3000 3001 8080 18080)
  local code=""
  local chosen_port=""
  local pf_pid=0
  local pf_log="/tmp/grafana-pf.log"

  for p in "${ports[@]}"; do
    # æ¸…ç†æ—§æ—¥å¿—å¹¶å¯åŠ¨ç«¯å£è½¬å‘
    : > "$pf_log"
    kubectl -n "$GRAFANA_NAMESPACE" port-forward svc/"$GRAFANA_RELEASE_NAME" "${p}:80" --address 127.0.0.1 >"$pf_log" 2>&1 &
    pf_pid=$!

    # ç­‰å¾…ç«¯å£è½¬å‘å°±ç»ªæˆ–å¤±è´¥ï¼ˆæœ€é•¿ ~10sï¼‰
    local wait_ok=0
    for i in {1..20}; do
      # ç«¯å£è½¬å‘æˆåŠŸæ—¶æ—¥å¿—ä¼šå‡ºç° "Forwarding from"
      if grep -q "Forwarding from" "$pf_log"; then
        wait_ok=1
        break
      fi
      # è‹¥è¿›ç¨‹å·²é€€å‡ºï¼Œè¯´æ˜è¯¥ç«¯å£å¯èƒ½è¢«å ç”¨æˆ–å…¶å®ƒé”™è¯¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç«¯å£
      if ! ps -p $pf_pid >/dev/null 2>&1; then
        break
      fi
      sleep 0.5
    done

    if [[ $wait_ok -ne 1 ]]; then
      kill $pf_pid 2>/dev/null || true
      wait $pf_pid 2>/dev/null || true
      continue
    fi

    # ç«¯å£è½¬å‘å·²å»ºç«‹ï¼Œå°è¯•å¤šæ¬¡è®¿é—® /api/health ç­‰å¾… Grafana å°±ç»ª
    for i in {1..15}; do
      code=$(curl -sS -o /dev/null -w '%{http_code}' --max-time 2 "http://127.0.0.1:${p}/api/health" || true)
      if [[ "$code" == "200" ]]; then
        chosen_port=$p
        break
      fi
      sleep 2
    done

    # æ— è®ºæˆåŠŸä¸å¦ï¼Œå…ˆå…³é—­å½“å‰ç«¯å£è½¬å‘
    kill $pf_pid 2>/dev/null || true
    if ps -p $pf_pid > /dev/null 2>&1; then
      wait $pf_pid 2>/dev/null || true
    fi

    # è‹¥æˆåŠŸæ‹¿åˆ° 200ï¼Œç»“æŸå¾ªç¯
    if [[ "$code" == "200" ]]; then
      break
    fi
  done

  if [[ "$code" == "200" ]]; then
    log "âœ… Grafana /api/health å¯è®¿é—®ï¼ˆç«¯å£ ${chosen_port}ï¼‰"
    return 0
  else
    # æ‰“å°æœ€åä¸€æ¬¡ç«¯å£è½¬å‘æ—¥å¿—å¸®åŠ©å®šä½é—®é¢˜
    log "âŒ Grafana /api/health è¿”å›ç : ${code:-000}"
    if [[ -s "$pf_log" ]]; then
      log "â„¹ï¸ kubectl port-forward æ—¥å¿—:"
      sed -n '1,50p' "$pf_log" || true
    fi
    return 1
  fi
}

# ä¸²è” task-api å„é¡¹æ£€æŸ¥
check_task_api() {
  log "ğŸ” æ£€æŸ¥ task-api"

  check_deployment_ready

  local fails=0
  local summary=()

  run_check() {
    local fn="$1"
    local label="$2"
    if ( "$fn" ); then
      summary+=("âœ… ${label}")
    else
      summary+=("âŒ ${label}")
      fails=$((fails+1))
    fi
  }

  run_check task_api_smoke_test "é›†ç¾¤å†…å†’çƒŸæµ‹è¯•"
  run_check verify_irsa_env "IRSA ç¯å¢ƒè‡ªæ£€"
  run_check check_pdb "PodDisruptionBudget"
  run_check check_ingress_alb "Ingress/ALB/DNS"
  run_check awscli_s3_smoke "aws-cli S3 æƒé™"
  run_check check_adot_ready "ADOT Collector"
  run_check check_grafana_ready "Grafana ç«¯åˆ°ç«¯"

  log "ğŸ“Š task-api æ£€æŸ¥ç»“æœæ±‡æ€»"
  for item in "${summary[@]}"; do
    log "$item"
  done

  if [[ $fails -gt 0 ]]; then
    abort "task-api æ£€æŸ¥å¤±è´¥ (${fails} é¡¹)"
  fi

  log "âœ… task-api æ£€æŸ¥å®Œæˆ"
}

# ---- ALBC Controller ----

# å®‰è£…æˆ–å‡çº§ AWS Load Balancer Controller
install_albc_controller() {
  local status
  status=$(check_albc_status)
  case "$status" in
    healthy)
      log "âœ… AWS Load Balancer Controller å·²æ­£å¸¸è¿è¡Œ, è·³è¿‡ Helm éƒ¨ç½²"
      return 0
      ;;
    missing)
      log "âš™ï¸  æ£€æµ‹åˆ° AWS Load Balancer Controller æœªéƒ¨ç½², å¼€å§‹å®‰è£…"
      ;;
    unhealthy)
      log "âŒ æ£€æµ‹åˆ° AWS Load Balancer Controller çŠ¶æ€å¼‚å¸¸, åˆ é™¤æ—§ Pod åé‡æ–°éƒ¨ç½²"
      kubectl -n $KUBE_DEFAULT_NAMESPACE delete pod -l $POD_ALBC_LABEL --ignore-not-found
      ;;
    *)
      log "âš ï¸  æœªçŸ¥çš„ AWS Load Balancer Controller çŠ¶æ€, ç»§ç»­å°è¯•å®‰è£…"
      ;;
  esac

  if ! helm repo list | grep -q "^${ALBC_HELM_REPO_NAME}\b"; then
    log "ğŸ”§ æ·»åŠ  ${ALBC_HELM_REPO_NAME} Helm ä»“åº“"
    helm repo add ${ALBC_HELM_REPO_NAME} ${ALBC_HELM_REPO_URL}
  fi
  helm repo update

  log "ğŸ“¦ åº”ç”¨ AWS Load Balancer Controller CRDs (version ${ALBC_CHART_VERSION})"
  tmp_dir=$(mktemp -d)
  helm pull ${ALBC_HELM_REPO_NAME}/${ALBC_CHART_NAME} --version ${ALBC_CHART_VERSION} --untar -d "$tmp_dir"
  kubectl apply -f "$tmp_dir/${ALBC_CHART_NAME}/crds"
  rm -rf "$tmp_dir"

  VPC_ID=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$REGION" --profile "$PROFILE" --query "cluster.resourcesVpcConfig.vpcId" --output text)

  log "ğŸš€ æ­£åœ¨é€šè¿‡ Helm å®‰è£…æˆ–å‡çº§ AWS Load Balancer Controller..."
  helm upgrade --install ${ALBC_RELEASE_NAME} ${ALBC_HELM_REPO_NAME}/${ALBC_CHART_NAME} \
    -n $KUBE_DEFAULT_NAMESPACE \
    --version ${ALBC_CHART_VERSION} \
    --set clusterName=$CLUSTER_NAME \
    --set region=$REGION \
    --set vpcId=$VPC_ID \
    --set serviceAccount.create=false \
    --set serviceAccount.name=${ALBC_SERVICE_ACCOUNT_NAME} \
    --set image.repository=${ALBC_IMAGE_REPO} \
    --set image.tag=${ALBC_IMAGE_TAG}

  log "ğŸ” ç­‰å¾… AWS Load Balancer Controller å°±ç»ª"
  kubectl -n $KUBE_DEFAULT_NAMESPACE rollout status deployment/${ALBC_RELEASE_NAME} --timeout=180s
  kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_ALBC_LABEL
}

# -- Cluster Autoscaler --

# å®‰è£…æˆ–å‡çº§ Cluster Autoscaler
install_autoscaler() {
  local status
  status=$(check_autoscaler_status)
  case "$status" in
    healthy)
      log "âœ… Cluster Autoscaler å·²æ­£å¸¸è¿è¡Œ, è·³è¿‡ Helm éƒ¨ç½²"
      return 0
      ;;
    missing)
      log "âš™ï¸  æ£€æµ‹åˆ° Cluster Autoscaler æœªéƒ¨ç½², å¼€å§‹å®‰è£…"
      ;;
    unhealthy)
      log "âŒ æ£€æµ‹åˆ° Cluster Autoscaler çŠ¶æ€å¼‚å¸¸, åˆ é™¤æ—§ Pod åé‡æ–°éƒ¨ç½²"
      kubectl -n $KUBE_DEFAULT_NAMESPACE delete pod -l $POD_AUTOSCALER_LABEL --ignore-not-found
      ;;
    *)
      log "âš ï¸  æœªçŸ¥çš„ Cluster Autoscaler çŠ¶æ€, ç»§ç»­å°è¯•å®‰è£…"
      ;;
  esac
  log "ğŸš€ æ­£åœ¨é€šè¿‡ Helm å®‰è£…æˆ–å‡çº§ Cluster Autoscaler..."
  if ! helm repo list | grep -q "^${AUTOSCALER_HELM_REPO_NAME}\b"; then
    log "ğŸ”§ æ·»åŠ  ${AUTOSCALER_HELM_REPO_NAME} Helm ä»“åº“"
    helm repo add ${AUTOSCALER_HELM_REPO_NAME} ${AUTOSCALER_HELM_REPO_URL}
  fi
  # è·å– Kubernetes å®Œæ•´ç‰ˆæœ¬ (å¦‚ v1.33.1)
  K8S_FULL_VERSION=$(kubectl version -o json | jq -r '.serverVersion.gitVersion')
  # æå–ä¸»æ¬¡ç‰ˆæœ¬å· (å¦‚ 1.33)
  K8S_MINOR_VERSION=$(echo "$K8S_FULL_VERSION" | sed -E 's/^v([0-9]+\.[0-9]+)\..*$/\1/')
  # å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›– Cluster Autoscaler ç‰ˆæœ¬ï¼Œå¦åˆ™é»˜è®¤ä½¿ç”¨ .0 è¡¥ä¸ç‰ˆæœ¬
  if [[ -z "${AUTOSCALER_VERSION:-}" ]]; then
    AUTOSCALER_VERSION="v${K8S_MINOR_VERSION}.0"
    log "âš ï¸  æœªè®¾ç½® AUTOSCALER_VERSIONï¼Œè‡ªåŠ¨æ¨æ–­ä¸º ${AUTOSCALER_VERSION}ã€‚å¦‚é‡ Helm æ‹‰å–å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æŒ‡å®šæ”¯æŒçš„ç‰ˆæœ¬ï¼ˆå¦‚ï¼šexport AUTOSCALER_VERSION=v1.33.0ï¼‰"
  else
    log "ğŸ“Œ ä½¿ç”¨æŒ‡å®šçš„ Cluster Autoscaler ç‰ˆæœ¬ï¼š${AUTOSCALER_VERSION}"
  fi
  helm upgrade --install ${AUTOSCALER_RELEASE_NAME} ${AUTOSCALER_HELM_REPO_NAME}/${AUTOSCALER_CHART_NAME} -n $KUBE_DEFAULT_NAMESPACE --create-namespace \
    --set awsRegion=$REGION \
    --set autoDiscovery.clusterName=$CLUSTER_NAME \
    --set rbac.serviceAccount.create=true \
    --set rbac.serviceAccount.name=${AUTOSCALER_SERVICE_ACCOUNT_NAME} \
    --set extraArgs.balance-similar-node-groups=true \
    --set extraArgs.skip-nodes-with-system-pods=false \
    --set rbac.serviceAccount.annotations."eks\\.amazonaws\\.com/role-arn"="$AUTOSCALER_ROLE_ARN" \
    --set image.tag=$AUTOSCALER_VERSION
  log "âœ… Helm install completed"
  log "ğŸ” æ£€æŸ¥ Cluster Autoscaler Pod çŠ¶æ€"
  kubectl -n $KUBE_DEFAULT_NAMESPACE rollout status deployment/${AUTOSCALER_DEPLOYMENT_NAME} --timeout=180s
  kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_AUTOSCALER_LABEL
}

# è·å–å½“å‰æœ€æ–° ASG å
get_latest_asg() {
  aws autoscaling describe-auto-scaling-groups \
    --region "$REGION" --profile "$PROFILE" \
    --query "AutoScalingGroups[?starts_with(AutoScalingGroupName, '$ASG_PREFIX')].AutoScalingGroupName" \
    --output text | head -n1
}

# -- SNS é€šçŸ¥ç»‘å®š --

# ç»‘å®š SNS é€šçŸ¥
bind_sns_notification() {
  local asg_name="$1"
  log "ğŸ”„ ç»‘å®š SNS é€šçŸ¥åˆ° ASG: $asg_name"
  aws autoscaling put-notification-configuration \
    --auto-scaling-group-name "$asg_name" \
    --topic-arn "$TOPIC_ARN" \
    --notification-types "autoscaling:EC2_INSTANCE_TERMINATE" \
    --region "$REGION" --profile "$PROFILE"
}

# ç¡®ä¿ SNS ç»‘å®šåˆ°æœ€æ–° ASG
ensure_sns_binding() {
  local asg_name="$1"
  if [[ -f "$STATE_FILE" ]]; then
    last_bound_asg=$(cat "$STATE_FILE")
  else
    last_bound_asg=""
  fi
  if [[ "$asg_name" == "$last_bound_asg" ]]; then
    log "âœ… å½“å‰ ASG [$asg_name] å·²ç»‘å®šè¿‡, æ— éœ€é‡å¤ç»‘å®š"
  else
    bind_sns_notification "$asg_name"
    echo "$asg_name" > "$STATE_FILE"
    log "âœ… å·²ç»‘å®šå¹¶è®°å½•æœ€æ–° ASG: $asg_name"
  fi
}

# === åŸºç¡€èµ„æºæ£€æŸ¥ ===

# æ£€æŸ¥ NAT ç½‘å…³çŠ¶æ€
check_nat_gateway() {
  aws ec2 describe-nat-gateways \
    --region "$REGION" --profile "$PROFILE" \
    --query "NatGateways[?State=='available']" --output json | jq length
}

# æ£€æŸ¥ ALB çŠ¶æ€
check_alb() {
  aws elbv2 describe-load-balancers \
    --region "$REGION" --profile "$PROFILE" \
    --query "LoadBalancers[?Type=='application']" --output json | jq length
}

# æ£€æŸ¥ EKS é›†ç¾¤çŠ¶æ€
check_eks_cluster() {
  aws eks describe-cluster \
    --region "$REGION" --profile "$PROFILE" \
    --name "$CLUSTER_NAME" \
    --query 'cluster.status' --output text
}

# æ£€æŸ¥èŠ‚ç‚¹ç»„çŠ¶æ€
check_nodegroup() {
  aws eks describe-nodegroup \
    --region "$REGION" --profile "$PROFILE" \
    --cluster-name "$CLUSTER_NAME" \
    --nodegroup-name "$NODEGROUP_NAME" \
    --query 'nodegroup.status' --output text
}

# æ£€æŸ¥æ—¥å¿—ç»„å­˜åœ¨
check_log_group() {
  aws logs describe-log-groups \
    --region "$REGION" --profile "$PROFILE" \
    --log-group-name-prefix "/aws/eks/${CLUSTER_NAME}/cluster" \
    --query 'logGroups[*].logGroupName' --output text
}

# æ£€æŸ¥ SNS ç»‘å®š
check_sns_binding() {
  local asg_name="$1"
  aws autoscaling describe-auto-scaling-groups \
    --region "$REGION" --profile "$PROFILE" \
    --auto-scaling-group-names "$asg_name" \
    --query "AutoScalingGroups[0].NotificationConfigurations[?TopicARN=='${TOPIC_ARN}']" \
    --output json | jq length
}

# è¿›è¡ŒåŸºç¡€èµ„æºæ£€æŸ¥
perform_health_checks() {
  local asg_name="$1"
  log "ğŸ” å¼€å§‹æ‰§è¡ŒåŸºç¡€èµ„æºå¥åº·æ£€æŸ¥..."
  log "ğŸ” æ£€æŸ¥ NAT ç½‘å…³çŠ¶æ€"
  nat_count=$(check_nat_gateway)
  log "NAT Gateway count: $nat_count"
  log "ğŸ” æ£€æŸ¥ ALB çŠ¶æ€"
  alb_count=$(check_alb)
  log "ALB count: $alb_count"
  log "ğŸ” æ£€æŸ¥ EKS é›†ç¾¤çŠ¶æ€"
  eks_status=$(check_eks_cluster)
  log "EKS cluster status: $eks_status"
  log "ğŸ” æ£€æŸ¥èŠ‚ç‚¹ç»„çŠ¶æ€"
  node_status=$(check_nodegroup)
  log "NodeGroup status: $node_status"
  log "ğŸ” æ£€æŸ¥ LogGroup æ˜¯å¦å­˜åœ¨"
  log_group=$(check_log_group)
  log "LogGroup: $log_group"
  log "ğŸ” æ£€æŸ¥ AWS Load Balancer Controller éƒ¨ç½²çŠ¶æ€"
  albc_status=$(check_albc_status)
  log "AWS Load Balancer Controller status: $albc_status"
  log "ğŸ” æ£€æŸ¥ Cluster Autoscaler éƒ¨ç½²çŠ¶æ€"
  autoscaler_status=$(check_autoscaler_status)
  log "Cluster Autoscaler status: $autoscaler_status"
  log "ğŸ” éªŒè¯ SNS é€šçŸ¥ç»‘å®š"
  sns_bound=$(check_sns_binding "$asg_name")
  log "SNS bindings for ASG [$asg_name]: $sns_bound"
}

# === éƒ¨ç½² task-api åˆ° EKSï¼ˆå¹‚ç­‰ï¼‰===

deploy_task_api() {
  # ===== å‰ç½®ï¼šAWS èº«ä»½ä¸ kubeconfig =====
  log "ğŸ” ä½¿ç”¨ profile=${PROFILE} region=${REGION}"
  ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text --profile "${PROFILE}")" || abort "æ— æ³•è·å– AWS è´¦å· ID"
  log "ğŸ‘¤ AWS Account: ${ACCOUNT_ID}"

  log "ğŸ”§ é…ç½® kubeconfigï¼ˆcluster=${CLUSTER_NAME}ï¼‰"
  aws eks update-kubeconfig --name "${CLUSTER_NAME}" --region "${REGION}" --profile "${PROFILE}" >/dev/null

  # ===== åº”ç”¨ Kubernetes æ¸…å• =====
  if [[ ! -d "${K8S_BASE_DIR}" ]]; then
    abort "æœªæ‰¾åˆ° k8s æ¸…å•ç›®å½•ï¼š${K8S_BASE_DIR}"
  fi
  log "ğŸ—‚ï¸  apply æ¸…å•ï¼šns-sa.yaml"
  kubectl -n "${NS}" apply -f "${K8S_BASE_DIR}/ns-sa.yaml"
  ensure_task_api_service_account  # ç¡®ä¿åº”ç”¨çº§ SA å¸¦ IRSA æ³¨è§£
  log "ğŸ—‚ï¸  apply æ¸…å•ï¼šconfigmap.yaml"
  kubectl -n "${NS}" apply -f "${K8S_BASE_DIR}/configmap.yaml"
  log "ğŸ—‚ï¸  apply æ¸…å•ï¼šdeploy-svc.yaml"
  kubectl -n "${NS}" apply -f "${K8S_BASE_DIR}/deploy-svc.yaml"
  log "ğŸ—‚ï¸  apply æ¸…å•ï¼špdb.yaml"
  # PodDisruptionBudget ç¡®ä¿åœ¨èŠ‚ç‚¹ç»´æŠ¤æˆ–æ»šåŠ¨å‡çº§ç­‰è‡ªæ„¿ä¸­æ–­åœºæ™¯ä¸‹ï¼Œè‡³å°‘ä¿ç•™ 1 ä¸ªå¯ç”¨ Pod
  kubectl -n "${NS}" apply -f "${K8S_BASE_DIR}/pdb.yaml"

  # ===== è§£æé•œåƒï¼ˆä¼˜å…ˆä½¿ç”¨å›ºå®š digestï¼‰=====
  if [[ -n "${IMAGE_DIGEST:-}" ]]; then
    DIGEST="${IMAGE_DIGEST}"
    log "ğŸ“Œ ä½¿ç”¨å›ºå®š digestï¼š${DIGEST}"
  else
    log "ğŸ” ä» ECR è·å– ${ECR_REPO}:${IMAGE_TAG} çš„ digest"
    set +e
    DIGEST="$(aws ecr describe-images \
      --repository-name "${ECR_REPO}" \
      --image-ids imageTag="${IMAGE_TAG}" \
      --query 'imageDetails[0].imageDigest' \
      --output text --region "${REGION}" --profile "${PROFILE}")"
    rc=$?
    set -e
    if [[ $rc -ne 0 || -z "${DIGEST}" || "${DIGEST}" == "None" ]]; then
      abort "ECR ä¸­æœªæ‰¾åˆ°é•œåƒ ${ECR_REPO}:${IMAGE_TAG} çš„ digestï¼Œè¯·å…ˆæ¨é€é•œåƒæˆ–è°ƒæ•´ IMAGE_TAG"
    fi
  fi
  IMAGE="${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com/${ECR_REPO}@${DIGEST}"

  # ===== è‹¥å·²éƒ¨ç½²ä¸”å¥åº·åˆ™è·³è¿‡é•œåƒæ›´æ–° =====
  skip_deploy=false
  current_image=""
  if kubectl -n "${NS}" get deploy "${APP}" >/dev/null 2>&1; then
    current_image=$(kubectl -n "${NS}" get deploy "${APP}" -o jsonpath='{.spec.template.spec.containers[0].image}' 2>/dev/null || echo "")
    if [[ "$current_image" == "${IMAGE}" ]]; then
      # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ Pod å‡ä¸º Running
      if ! kubectl -n "${NS}" get pods -l app="${APP}" --no-headers 2>/dev/null | grep -v Running >/dev/null; then
        log "âœ… é•œåƒ ${IMAGE} å·²éƒ¨ç½²ä¸”è¿è¡Œæ­£å¸¸ï¼Œè·³è¿‡é•œåƒæ›´æ–°"
        skip_deploy=true
      else
        log "âš ï¸ é•œåƒä¸€è‡´ä½†å­˜åœ¨å¼‚å¸¸ Podï¼Œé‡æ–°éƒ¨ç½²"
      fi
    fi
  fi

  if [[ "${skip_deploy}" != true ]]; then
    # ===== ç”¨ set image è¦†ç›–é•œåƒï¼Œå¹¶è®°å½• rollout å†å² =====
    log "ğŸ–¼ï¸  å°†éƒ¨ç½²é•œåƒï¼š${IMAGE}"
    log "â™»ï¸  æ›´æ–° Deployment é•œåƒå¹¶ç­‰å¾…æ»šåŠ¨å®Œæˆ"
    kubectl -n "${NS}" set image deploy/"${APP}" "${APP}"="${IMAGE}" --record
    kubectl -n "${NS}" rollout status deploy/"${APP}" --timeout=180s
    kubectl -n "${NS}" get deploy,svc -o wide
  fi
}

# éƒ¨ç½² taskapi ingress
deploy_task_api_ingress() {
  log "ğŸ“¦ Apply Ingress (${APP}) ..."
  # è‹¥æ— å˜æ›´å°±ä¸ applyï¼ˆ0=æ— å·®å¼‚ï¼Œ1=æœ‰å·®å¼‚ï¼Œ>1=å‡ºé”™ï¼‰
  if kubectl -n "$NS" diff -f "$ING_FILE" >/dev/null 2>&1; then
    log "â‰¡ No changes"
  else
    kubectl -n "$NS" apply -f "$ING_FILE"
  fi
}

### ---- metrics-server (Helm) ----

deploy_metrics_server() {
  log "ğŸ” æ£€æŸ¥ metrics-server çŠ¶æ€..."
  if kubectl -n "$KUBE_DEFAULT_NAMESPACE" get deployment metrics-server >/dev/null 2>&1; then
    local replicas available
    replicas=$(kubectl -n "$KUBE_DEFAULT_NAMESPACE" get deployment metrics-server -o jsonpath='{.status.replicas}')
    available=$(kubectl -n "$KUBE_DEFAULT_NAMESPACE" get deployment metrics-server -o jsonpath='{.status.availableReplicas}')
    replicas=${replicas:-0}
    available=${available:-0}
    if [[ "$replicas" -gt 0 && "$replicas" == "$available" ]]; then
      log "âœ… metrics-server å·²éƒ¨ç½²ä¸”å¥åº·ï¼Œè·³è¿‡å®‰è£…"
      return
    fi
    log "âš ï¸ metrics-server å­˜åœ¨ä½†æœªå°±ç»ªï¼Œé‡æ–°éƒ¨ç½²"
  fi

  log "ğŸ“¦ Installing metrics-server ..."
  helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/ >/dev/null 2>&1 || true
  helm repo update >/dev/null 2>&1
  helm upgrade --install metrics-server metrics-server/metrics-server \
    --namespace "$KUBE_DEFAULT_NAMESPACE" \
    --version 3.12.1 \
    --set args={--kubelet-insecure-tls}
  kubectl -n "$KUBE_DEFAULT_NAMESPACE" rollout status deploy/metrics-server --timeout=180s
}

### ---- HPA for task-api ----

deploy_taskapi_hpa() {
  log "ğŸ“¦ Apply HPA for task-api ..."
  kubectl -n "$NS" apply -f "$HPA_FILE"
  log "ğŸ” Describe HPA"
  kubectl -n "$NS" describe hpa task-api | sed -n '1,60p' || true
}

### ---- ADOT Collector + AMP (Helm) ----

check_adot_status() {
  # returns: healthy|missing|unhealthy
  if ! kubectl -n "$ADOT_NAMESPACE" get deployment "$ADOT_DEPLOYMENT_NAME" >/dev/null 2>&1; then
    echo "missing"; return
  fi
  if kubectl -n "$ADOT_NAMESPACE" get pod -l app.kubernetes.io/instance="${ADOT_RELEASE_NAME}" --no-headers 2>/dev/null | grep -v Running >/dev/null; then
    echo "unhealthy"
  else
    echo "healthy"
  fi
}

deploy_adot_collector() {
  log "ğŸ” å‡†å¤‡éƒ¨ç½² ADOT Collector åˆ°å‘½åç©ºé—´: ${ADOT_NAMESPACE}"
  if ! kubectl get ns "${ADOT_NAMESPACE}" >/dev/null 2>&1; then
    log "ğŸ§± åˆ›å»ºå‘½åç©ºé—´ ${ADOT_NAMESPACE}"
    kubectl create namespace "${ADOT_NAMESPACE}"
  fi

  if [[ ! -f "${ADOT_VALUES_FILE}" ]]; then
    abort "ç¼ºå°‘ Helm values æ–‡ä»¶: ${ADOT_VALUES_FILE}"
  fi

  # Skip when already healthy
  local cur_status
  cur_status=$(check_adot_status || true)
  if [[ "$cur_status" == "healthy" ]]; then
    log "âœ… ADOT Collector å·²éƒ¨ç½²ä¸”å¥åº·ï¼Œè·³è¿‡ Helm å‡çº§"
    # still validate IRSA annotation
    local sa_arn
    sa_arn=$(kubectl -n "$ADOT_NAMESPACE" get sa "$ADOT_SERVICE_ACCOUNT_NAME" -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}' 2>/dev/null || true)
    [[ "$sa_arn" == "$ADOT_ROLE_ARN" ]] || abort "ADOT ServiceAccount æ³¨è§£ç¼ºå¤±æˆ–ä¸åŒ¹é… (got='${sa_arn}' expected='${ADOT_ROLE_ARN}')"
    return 0
  fi

  if ! helm repo list | grep -q "^${ADOT_HELM_REPO_NAME}\\b"; then
    log "ğŸ”§ æ·»åŠ  ${ADOT_HELM_REPO_NAME} Helm ä»“åº“"
    helm repo add ${ADOT_HELM_REPO_NAME} ${ADOT_HELM_REPO_URL}
  fi
  helm repo update >/dev/null 2>&1 || true

  log "ğŸš€ é€šè¿‡ Helm å®‰è£…/å‡çº§ ADOT Collector (${ADOT_RELEASE_NAME})"
  helm upgrade --install "${ADOT_RELEASE_NAME}" ${ADOT_HELM_REPO_NAME}/opentelemetry-collector \
    -n "${ADOT_NAMESPACE}" --create-namespace \
    -f "${ADOT_VALUES_FILE}"

  log "â³ ç­‰å¾… ADOT Collector Deployment (${ADOT_DEPLOYMENT_NAME}) å°±ç»ª"
  if ! kubectl -n "${ADOT_NAMESPACE}" rollout status deployment/"${ADOT_DEPLOYMENT_NAME}" --timeout=180s; then
    kubectl -n "${ADOT_NAMESPACE}" get pods -l app.kubernetes.io/instance="${ADOT_RELEASE_NAME}" || true
    abort "ADOT Collector æœªåœ¨ 180s å†…å°±ç»ª"
  fi
  kubectl -n "${ADOT_NAMESPACE}" get pods -l app.kubernetes.io/instance="${ADOT_RELEASE_NAME}" || true
}

### ---- Grafana (Helm) ----

check_grafana_status() {
  # returns: healthy|missing|unhealthy
  if ! kubectl -n "$GRAFANA_NAMESPACE" get deployment "$GRAFANA_RELEASE_NAME" >/dev/null 2>&1; then
    echo "missing"; return
  fi
  if kubectl -n "$GRAFANA_NAMESPACE" get pod -l app.kubernetes.io/instance="${GRAFANA_RELEASE_NAME}" --no-headers 2>/dev/null | grep -v Running >/dev/null; then
    echo "unhealthy"
  else
    echo "healthy"
  fi
}

deploy_grafana() {
  log "ğŸ” å‡†å¤‡éƒ¨ç½² Grafana åˆ°å‘½åç©ºé—´: ${GRAFANA_NAMESPACE}"
  if ! kubectl get ns "${GRAFANA_NAMESPACE}" >/dev/null 2>&1; then
    log "ğŸ§± åˆ›å»ºå‘½åç©ºé—´ ${GRAFANA_NAMESPACE}"
    kubectl create namespace "${GRAFANA_NAMESPACE}"
  fi

  if [[ ! -f "${GRAFANA_VALUES_FILE}" ]]; then
    abort "ç¼ºå°‘ Helm values æ–‡ä»¶: ${GRAFANA_VALUES_FILE}"
  fi

  local cur_status
  cur_status=$(check_grafana_status || true)
  if [[ "$cur_status" == "healthy" ]]; then
    log "âœ… Grafana å·²éƒ¨ç½²ä¸”å¥åº·ï¼Œè·³è¿‡ Helm å‡çº§"
    return 0
  elif [[ "$cur_status" == "unhealthy" ]]; then
    log "âš ï¸ Grafana å­˜åœ¨ä½†æœªå°±ç»ªï¼Œé‡æ–°éƒ¨ç½²"
  fi

  if ! helm repo list | grep -q "^${GRAFANA_HELM_REPO_NAME}\\b"; then
    log "ğŸ”§ æ·»åŠ  ${GRAFANA_HELM_REPO_NAME} Helm ä»“åº“"
    helm repo add ${GRAFANA_HELM_REPO_NAME} ${GRAFANA_HELM_REPO_URL}
  fi
  helm repo update >/dev/null 2>&1 || true

  log "ğŸš€ é€šè¿‡ Helm å®‰è£…/å‡çº§ Grafana (${GRAFANA_RELEASE_NAME})"
  helm upgrade --install "${GRAFANA_RELEASE_NAME}" ${GRAFANA_HELM_REPO_NAME}/grafana \
    -n "${GRAFANA_NAMESPACE}" --create-namespace \
    -f "${GRAFANA_VALUES_FILE}"

  log "â³ ç­‰å¾… Grafana Deployment å°±ç»ª"
  if ! kubectl -n "${GRAFANA_NAMESPACE}" rollout status deployment/"${GRAFANA_RELEASE_NAME}" --timeout=180s; then
    kubectl -n "${GRAFANA_NAMESPACE}" get pods -l app.kubernetes.io/instance="${GRAFANA_RELEASE_NAME}" || true
    abort "Grafana æœªåœ¨ 180s å†…å°±ç»ª"
  fi
  kubectl -n "${GRAFANA_NAMESPACE}" get pods -l app.kubernetes.io/instance="${GRAFANA_RELEASE_NAME}" || true
}

### ---- Chaos Meshï¼ˆHelm, å¯é€‰ï¼‰----

# è¿”å› Chaos Mesh å®‰è£…çŠ¶æ€ï¼šhealthy|missing|unhealthy
check_chaos_mesh_status() {
  if ! kubectl -n "$CHAOS_NAMESPACE" get deployment "$CHAOS_DEPLOYMENT_NAME" >/dev/null 2>&1; then
    echo "missing"; return
  fi
  if kubectl -n "$CHAOS_NAMESPACE" get pod -l app.kubernetes.io/instance="${CHAOS_RELEASE_NAME}" --no-headers 2>/dev/null | grep -v Running >/dev/null; then
    echo "unhealthy"
  else
    echo "healthy"
  fi
}

# éƒ¨ç½² Chaos Meshï¼ˆå¯é€‰ï¼‰
deploy_chaos_mesh() {
  if [[ "$ENABLE_CHAOS_MESH" != "true" ]]; then
    log "â„¹ï¸ æœªå¯ç”¨ ENABLE_CHAOS_MESHï¼Œè·³è¿‡éƒ¨ç½² Chaos Mesh"
    return 0
  fi

  log "ğŸ” å‡†å¤‡éƒ¨ç½² Chaos Mesh åˆ°å‘½åç©ºé—´: ${CHAOS_NAMESPACE}"

  if [[ ! -f "${CHAOS_VALUES_FILE}" ]]; then
    abort "ç¼ºå°‘ Helm values æ–‡ä»¶: ${CHAOS_VALUES_FILE}"
  fi

  # å¦‚å·²å¥åº·ï¼Œç›´æ¥è·³è¿‡å®‰è£…/å‡çº§
  local cur_status
  cur_status=$(check_chaos_mesh_status || true)
  if [[ "$cur_status" == "healthy" ]]; then
    log "âœ… Chaos Mesh å·²éƒ¨ç½²ä¸”å¥åº·ï¼Œè·³è¿‡ Helm å‡çº§"
    return 0
  elif [[ "$cur_status" == "unhealthy" ]]; then
    log "âš ï¸ Chaos Mesh å­˜åœ¨ä½†æœªå°±ç»ªï¼Œæ‰§è¡Œ Helm å‡çº§ä»¥è‡ªæ„ˆ"
  else
    log "â„¹ï¸ Chaos Mesh æœªå®‰è£…ï¼Œå¼€å§‹å®‰è£…"
  fi

  if ! helm repo list | grep -q "^${CHAOS_HELM_REPO_NAME}\\b"; then
    log "ğŸ”§ æ·»åŠ  ${CHAOS_HELM_REPO_NAME} Helm ä»“åº“"
    helm repo add ${CHAOS_HELM_REPO_NAME} ${CHAOS_HELM_REPO_URL}
  fi
  helm repo update >/dev/null 2>&1 || true

  log "ğŸš€ é€šè¿‡ Helm å®‰è£…/å‡çº§ Chaos Mesh (${CHAOS_RELEASE_NAME})"
  helm upgrade --install "${CHAOS_RELEASE_NAME}" ${CHAOS_HELM_REPO_NAME}/chaos-mesh \
    -n "${CHAOS_NAMESPACE}" --create-namespace \
    -f "${CHAOS_VALUES_FILE}"

  log "â³ ç­‰å¾… Chaos Mesh Controller å°±ç»ª"
  if ! kubectl -n "${CHAOS_NAMESPACE}" rollout status deployment/"$CHAOS_DEPLOYMENT_NAME" --timeout=180s; then
    kubectl -n "${CHAOS_NAMESPACE}" get pods -l app.kubernetes.io/instance="${CHAOS_RELEASE_NAME}" || true
    abort "Chaos Mesh controller æœªåœ¨ 180s å†…å°±ç»ª"
  fi
  kubectl -n "${CHAOS_NAMESPACE}" get pods -l app.kubernetes.io/instance="${CHAOS_RELEASE_NAME}" || true
}

# === ä¸»æµç¨‹ ===
log "ğŸ“£ å¼€å§‹æ‰§è¡Œ post-recreate è„šæœ¬"

if ! cluster_exists; then
  log "âš ï¸  æœªæ‰¾åˆ° EKS é›†ç¾¤ $CLUSTER_NAMEï¼Œå¯èƒ½å·²é”€æ¯ï¼Œè„šæœ¬é€€å‡º"
  exit 0
fi

log "ğŸ” è·å–æœ€æ–°çš„ ASG åç§°"
asg_name=$(get_latest_asg)
if [[ -z "$asg_name" ]]; then
  abort "æœªæ‰¾åˆ°ä»¥ $ASG_PREFIX å¼€å¤´çš„ ASG, ç»ˆæ­¢è„šæœ¬"
fi

update_kubeconfig

wait_cluster_ready  # ç¡®ä¿é›†ç¾¤ API å°±ç»ªï¼Œé¿å…åç»­æ“ä½œè¶…æ—¶

ensure_albc_service_account

install_albc_controller

install_autoscaler

ensure_sns_binding "$asg_name"

perform_health_checks "$asg_name"

deploy_task_api

deploy_task_api_ingress

deploy_metrics_server

deploy_adot_collector

deploy_grafana

deploy_taskapi_hpa

deploy_chaos_mesh

check_task_api
