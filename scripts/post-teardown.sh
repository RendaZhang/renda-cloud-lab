#!/usr/bin/env bash
# ------------------------------------------------------------
# Renda Cloud Lab Â· post-teardown.sh
# åŠŸèƒ½: åœ¨ IaC é”€æ¯åå…œåº•æ¸…ç†ä»å¯èƒ½è®¡è´¹çš„èµ„æºï¼ˆALB/TargetGroup/SG ç­‰ï¼‰
# Usage:
#   bash scripts/post-teardown.sh
#   DRY_RUN=true bash scripts/post-teardown.sh   # é¢„æ¼”ï¼Œä¸æ‰§è¡Œåˆ é™¤
# ------------------------------------------------------------
set -euo pipefail

# ===== å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›– =====
REGION="${REGION:-us-east-1}"
PROFILE="${PROFILE:-phase2-sso}"
CLUSTER_NAME="${CLUSTER_NAME:-dev}"

# å¯é€‰ï¼šåç§°/å‰ç¼€ï¼ˆä¿ç•™ä½ çš„åŸæœ‰å˜é‡ï¼‰
LOG_GROUP="${LOG_GROUP:-/aws/eks/${CLUSTER_NAME}/cluster}"
NAT_NAME="${NAT_NAME:-lab-nat}"
ASG_PREFIX="${ASG_PREFIX:-eks-ng-mixed}"

# é¢„æ¼”æ¨¡å¼ï¼ˆåªæ‰“å°ä¸åˆ ï¼‰
DRY_RUN="${DRY_RUN:-false}"

log() { echo "[$(date '+%Y-%m-%d %H:%M:%S')] $*"; }
run() {
  if [[ "$DRY_RUN" == "true" ]]; then
    log "DRY-RUN: $*"
  else
    eval "$@"
  fi
}

# ---------- åŸºç¡€æ¢æµ‹ ----------
cluster_exists() {
  aws eks describe-cluster \
    --name "$CLUSTER_NAME" --region "$REGION" --profile "$PROFILE" >/dev/null 2>&1
}

# ---------- CloudWatch æ—¥å¿—ç»„ ----------
delete_log_group() {
  log "ğŸ§¹ æ¸…ç† CloudWatch Log Group: $LOG_GROUP"
  if aws logs describe-log-groups \
        --log-group-name-prefix "$LOG_GROUP" \
        --region "$REGION" --profile "$PROFILE" \
        | grep -q "\"logGroupName\": \"$LOG_GROUP\""; then
    run "aws logs delete-log-group --log-group-name \"$LOG_GROUP\" --region \"$REGION\" --profile \"$PROFILE\""
    log "âœ… å·²åˆ é™¤æ—¥å¿—ç»„ $LOG_GROUP"
  else
    log "â„¹ï¸ æ—¥å¿—ç»„ $LOG_GROUP ä¸å­˜åœ¨ï¼Œè·³è¿‡"
  fi
}

# ---------- å…œåº•åˆ é™¤ï¼šALB / TargetGroup / SG ----------
# è¯´æ˜ï¼š
# - ä¾æ®æ ‡ç­¾åˆ é™¤ï¼Œé¿å…è¯¯ä¼¤ã€‚åŒ¹é…ä»¥ä¸‹ä¸¤ç±»æ ‡ç­¾ä»»æ„å…¶ä¸€å³è§†ä¸ºæœ¬é›†ç¾¤èµ„æºï¼š
#   * elbv2.k8s.aws/cluster = $CLUSTER_NAME
#   * kubernetes.io/cluster/$CLUSTER_NAME = (owned|shared)
delete_alb_and_tg_for_cluster() {
  log "ğŸ§¹ æ‰«æå¹¶åˆ é™¤å±äºé›†ç¾¤ ${CLUSTER_NAME} çš„ ALB ä¸ TargetGroups ..."

  # åˆ—å‡ºæ‰€æœ‰ ALB ARNs
  mapfile -t LB_ARNS < <(aws elbv2 describe-load-balancers \
    --region "$REGION" --profile "$PROFILE" \
    --query 'LoadBalancers[].LoadBalancerArn' --output text 2>/dev/null | tr '\t' '\n' | sed '/^$/d')

  local to_delete_lbs=()
  for arn in "${LB_ARNS[@]:-}"; do
    # è¯»å–æ ‡ç­¾
    local tags_json
    tags_json=$(aws elbv2 describe-tags --resource-arns "$arn" \
      --region "$REGION" --profile "$PROFILE" 2>/dev/null || echo '{}')

    # åˆ¤æ–­æ˜¯å¦å±äºæœ¬é›†ç¾¤
    local owned
    owned=$(jq -r --arg c "$CLUSTER_NAME" '
      .TagDescriptions[0].Tags as $t
      | ([$t[] | select(.Key=="elbv2.k8s.aws/cluster" and .Value==$c)] | length > 0)
        or
        ([$t[] | select(.Key=="kubernetes.io/cluster/"+$c)] | length > 0)
      ' <<<"$tags_json" 2>/dev/null || echo "false")

    if [[ "$owned" == "true" ]]; then
      to_delete_lbs+=("$arn")
    fi
  done

  if [[ ${#to_delete_lbs[@]} -eq 0 ]]; then
    log "â„¹ï¸ æœªå‘ç°ä¸é›†ç¾¤ ${CLUSTER_NAME} ç›¸å…³çš„ ALB"
  else
    log "ğŸ” å‘ç° ${#to_delete_lbs[@]} ä¸ª ALB å¾…åˆ é™¤"
    for lb_arn in "${to_delete_lbs[@]}"; do
      local lb_name
      lb_name=$(aws elbv2 describe-load-balancers --load-balancer-arns "$lb_arn" \
        --region "$REGION" --profile "$PROFILE" \
        --query 'LoadBalancers[0].LoadBalancerName' --output text 2>/dev/null || echo "$lb_arn")
      log "â¡ï¸ åˆ é™¤ ALB: $lb_name"

      # è®°å½•ä¸è¯¥ LB å…³è”çš„ Target Groupsï¼ŒLB åˆ é™¤åå†åˆ  TG
      mapfile -t LBTGS < <(aws elbv2 describe-target-groups \
        --load-balancer-arn "$lb_arn" \
        --region "$REGION" --profile "$PROFILE" \
        --query 'TargetGroups[].TargetGroupArn' --output text 2>/dev/null | tr '\t' '\n' | sed '/^$/d')

      run "aws elbv2 delete-load-balancer --load-balancer-arn \"$lb_arn\" --region \"$REGION\" --profile \"$PROFILE\""

      # ç­‰å¾… LB åˆ é™¤å®Œæˆï¼ˆè½®è¯¢ 30 æ¬¡ï¼Œæ¯æ¬¡ 5 ç§’ï¼‰
      for i in {1..30}; do
        if aws elbv2 describe-load-balancers --load-balancer-arns "$lb_arn" \
             --region "$REGION" --profile "$PROFILE" >/dev/null 2>&1; then
          sleep 5
        else
          break
        fi
      done

      # åˆ é™¤ä¸ä¹‹å…³è”çš„ Target Groups
      for tg in "${LBTGS[@]:-}"; do
        log "   â†³ åˆ é™¤ TargetGroup: $tg"
        run "aws elbv2 delete-target-group --target-group-arn \"$tg\" --region \"$REGION\" --profile \"$PROFILE\" || true"
      done
    done
  fi

  # å†æ¬¡å…œåº•ï¼šæ¸…é™¤â€œå±äºæœ¬é›†ç¾¤ä¸”æœªè¢«ä½¿ç”¨â€çš„ TG
  log "ğŸ§¹ æ¸…ç†é—ç•™çš„ã€å¸¦é›†ç¾¤æ ‡ç­¾çš„å­¤å„¿ TargetGroups ..."
  mapfile -t TG_ARNS < <(aws elbv2 describe-target-groups \
    --region "$REGION" --profile "$PROFILE" \
    --query 'TargetGroups[].TargetGroupArn' --output text 2>/dev/null | tr '\t' '\n' | sed '/^$/d')

  for tg_arn in "${TG_ARNS[@]:-}"; do
    local tg_desc tg_lbs tg_tags_json attach_cnt is_owned
    tg_desc=$(aws elbv2 describe-target-groups --target-group-arns "$tg_arn" \
      --region "$REGION" --profile "$PROFILE" 2>/dev/null || echo '{}')
    attach_cnt=$(jq -r '.TargetGroups[0].LoadBalancerArns | length' <<<"$tg_desc" 2>/dev/null || echo "0")

    tg_tags_json=$(aws elbv2 describe-tags --resource-arns "$tg_arn" \
      --region "$REGION" --profile "$PROFILE" 2>/dev/null || echo '{}')
    is_owned=$(jq -r --arg c "$CLUSTER_NAME" '
      .TagDescriptions[0].Tags as $t
      | ([$t[] | select(.Key=="elbv2.k8s.aws/cluster" and .Value==$c)] | length > 0)
        or
        ([$t[] | select(.Key=="kubernetes.io/cluster/"+$c)] | length > 0)
      ' <<<"$tg_tags_json" 2>/dev/null || echo "false")

    if [[ "$is_owned" == "true" && "$attach_cnt" -eq 0 ]]; then
      log "   â†³ åˆ é™¤å­¤å„¿ TargetGroup: $tg_arn"
      run "aws elbv2 delete-target-group --target-group-arn \"$tg_arn\" --region \"$REGION\" --profile \"$PROFILE\" || true"
    fi
  done
}

delete_alb_security_groups() {
  log "ğŸ§¹ æ¸…ç†ç”± ALB Controller åˆ›å»ºã€å¹¶æ‰“äº†é›†ç¾¤æ ‡ç­¾çš„å®‰å…¨ç»„ ..."

  # 1) é€šè¿‡ elbv2.k8s.aws/cluster=<cluster> æ ‡ç­¾ç­›é€‰
  mapfile -t SG1 < <(aws ec2 describe-security-groups \
    --region "$REGION" --profile "$PROFILE" \
    --filters "Name=tag:elbv2.k8s.aws/cluster,Values=${CLUSTER_NAME}" \
    --query 'SecurityGroups[].GroupId' --output text 2>/dev/null | tr '\t' '\n' | sed '/^$/d')

  # 2) é€šè¿‡ kubernetes.io/cluster/<cluster> æ ‡ç­¾é”®ç­›é€‰
  mapfile -t SG2 < <(aws ec2 describe-security-groups \
    --region "$REGION" --profile "$PROFILE" \
    --filters "Name=tag-key,Values=kubernetes.io/cluster/${CLUSTER_NAME}" \
    --query 'SecurityGroups[].GroupId' --output text 2>/dev/null | tr '\t' '\n' | sed '/^$/d')

  # å»é‡
  declare -A SEEN; local sgs=()
  for id in "${SG1[@]:-}" "${SG2[@]:-}"; do
    [[ -n "${id:-}" && -z "${SEEN[$id]:-}" ]] && SEEN[$id]=1 && sgs+=("$id")
  done

  if [[ ${#sgs[@]} -eq 0 ]]; then
    log "â„¹ï¸ æœªå‘ç°éœ€è¦æ¸…ç†çš„å®‰å…¨ç»„"
    return
  fi

  for sg in "${sgs[@]}"; do
    # ç¡®ä¿ SG æœªè¢«å¼•ç”¨/æœªé™„ç€ ENIï¼Œå¦åˆ™åˆ é™¤ä¼šå¤±è´¥
    local attached
    attached=$(aws ec2 describe-network-interfaces \
      --region "$REGION" --profile "$PROFILE" \
      --filters "Name=group-id,Values=${sg}" \
      --query 'NetworkInterfaces | length(@)' --output text 2>/dev/null || echo "0")

    if [[ "$attached" != "0" ]]; then
      log "   âš ï¸ å®‰å…¨ç»„ $sg ä»è¢« ${attached} ä¸ª ENI å¼•ç”¨ï¼Œè·³è¿‡"
      continue
    fi

    log "   â†³ åˆ é™¤å®‰å…¨ç»„: $sg"
    run "aws ec2 delete-security-group --group-id \"$sg\" --region \"$REGION\" --profile \"$PROFILE\" || true"
  done
}

# ---------- åªåšæ£€æŸ¥ï¼ˆä¸åˆ ï¼‰çš„ä¿ç•™é¡¹ ----------
check_nat_gateway_deleted() {
  log "ğŸ” æ£€æŸ¥ NAT ç½‘å…³ ${NAT_NAME} æ˜¯å¦å·²åˆ é™¤"
  local count
  count=$(aws ec2 describe-nat-gateways \
    --region "$REGION" --profile "$PROFILE" \
    --filter "Name=tag:Name,Values=${NAT_NAME}" \
    --query "NatGateways[?State!='deleted'] | length(@)" --output text 2>/dev/null || echo "0")
  if [[ "$count" == "0" ]]; then
    log "âœ… NAT ç½‘å…³å·²åˆ é™¤"
  else
    log "âŒ NAT ç½‘å…³ä»å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ IaC æˆ–æ§åˆ¶å°"
  fi
}

check_eks_cluster_deleted() {
  log "ğŸ” æ£€æŸ¥ EKS é›†ç¾¤ ${CLUSTER_NAME} æ˜¯å¦å·²åˆ é™¤"
  if aws eks describe-cluster --name "$CLUSTER_NAME" \
       --region "$REGION" --profile "$PROFILE" >/dev/null 2>&1; then
    log "âŒ EKS é›†ç¾¤ä»å­˜åœ¨"
  else
    log "âœ… EKS é›†ç¾¤å·²åˆ é™¤"
  fi
}

check_sns_unbound() {
  log "ğŸ” æ£€æŸ¥ ASG çš„ SNS é€šçŸ¥æ˜¯å¦è§£ç»‘ï¼ˆå‰ç¼€ï¼š${ASG_PREFIX}ï¼‰"
  local asgs
  asgs=$(aws autoscaling describe-auto-scaling-groups \
    --region "$REGION" --profile "$PROFILE" \
    --query "AutoScalingGroups[?starts_with(AutoScalingGroupName, '${ASG_PREFIX}')].AutoScalingGroupName" \
    --output text 2>/dev/null)
  if [[ -z "${asgs:-}" ]]; then
    log "âœ… æœªæ‰¾åˆ°åŒ¹é…çš„ ASG, SNS ç»‘å®šå·²è§£é™¤"
  else
    local asg
    for asg in $asgs; do
      local ncount
      ncount=$(aws autoscaling describe-auto-scaling-groups \
        --region "$REGION" --profile "$PROFILE" \
        --auto-scaling-group-names "$asg" \
        --query 'AutoScalingGroups[0].NotificationConfigurations | length(@)' \
        --output text 2>/dev/null || echo "0")
      if [[ "$ncount" == "0" ]]; then
        log "âœ… ASG ${asg} æœªé…ç½®é€šçŸ¥"
      else
        log "âŒ ASG ${asg} ä»å­˜åœ¨é€šçŸ¥ç»‘å®š"
      fi
    done
  fi
}

# ========== ä¸»æµç¨‹ ==========
if cluster_exists; then
  log "âš ï¸ æ£€æµ‹åˆ° EKS é›†ç¾¤ ${CLUSTER_NAME} ä»å­˜åœ¨ï¼Œç–‘ä¼¼æœªæ‰§è¡Œé”€æ¯æ“ä½œï¼›ä¸ºé¿å…è¯¯åˆ ï¼Œè„šæœ¬é€€å‡ºã€‚"
  exit 0
fi

delete_log_group
delete_alb_and_tg_for_cluster
delete_alb_security_groups
check_nat_gateway_deleted
check_eks_cluster_deleted
check_sns_unbound
log "âœ… Post teardown cleanup completed"
